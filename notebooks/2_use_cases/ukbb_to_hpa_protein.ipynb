{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKBB to HPA Protein Mapping\n",
    "\n",
    "This notebook performs the initial setup and investigation for mapping proteins between the UK Biobank (UKBB) and Human Protein Atlas (HPA) datasets using the Biomapper framework.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and explore the UKBB and HPA protein datasets\n",
    "2. Configure data sources in the protein_config.yaml\n",
    "3. Resolve UniProt IDs using historical resolver\n",
    "4. Find the overlap between the two datasets\n",
    "\n",
    "Created: 2025-01-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Investigation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKBB Protein Metadata:\n",
      "Shape: (2923, 3)\n",
      "Columns: ['Assay', 'UniProt', 'Panel']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assay</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>Panel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARSD1</td>\n",
       "      <td>Q9BTE6</td>\n",
       "      <td>Oncology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABHD14B</td>\n",
       "      <td>Q96IU4</td>\n",
       "      <td>Neurology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABL1</td>\n",
       "      <td>P00519</td>\n",
       "      <td>Oncology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACAA1</td>\n",
       "      <td>P09110</td>\n",
       "      <td>Oncology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACAN</td>\n",
       "      <td>P16112</td>\n",
       "      <td>Cardiometabolic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Assay UniProt            Panel\n",
       "0   AARSD1  Q9BTE6         Oncology\n",
       "1  ABHD14B  Q96IU4        Neurology\n",
       "2     ABL1  P00519         Oncology\n",
       "3    ACAA1  P09110         Oncology\n",
       "4     ACAN  P16112  Cardiometabolic"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load UKBB protein metadata\n",
    "ukbb_path = \"/procedure/data/local_data/MAPPING_ONTOLOGIES/ukbb/UKBB_Protein_Meta.tsv\"\n",
    "ukbb_df = pd.read_csv(ukbb_path, sep='\\t')\n",
    "\n",
    "print(\"UKBB Protein Metadata:\")\n",
    "print(f\"Shape: {ukbb_df.shape}\")\n",
    "print(f\"Columns: {list(ukbb_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "ukbb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPA Organ-Specific Proteins:\n",
      "Shape: (3018, 3)\n",
      "Columns: ['gene', 'uniprot', 'organ']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>uniprot</th>\n",
       "      <th>organ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CFH</td>\n",
       "      <td>P08603</td>\n",
       "      <td>liver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALS2</td>\n",
       "      <td>Q96Q42</td>\n",
       "      <td>brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCB5</td>\n",
       "      <td>Q2M3G0</td>\n",
       "      <td>epididymis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SLC25A13</td>\n",
       "      <td>Q9UJS0</td>\n",
       "      <td>liver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SLC4A1</td>\n",
       "      <td>P02730</td>\n",
       "      <td>bone marrow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gene uniprot        organ\n",
       "0       CFH  P08603        liver\n",
       "1      ALS2  Q96Q42        brain\n",
       "2     ABCB5  Q2M3G0   epididymis\n",
       "3  SLC25A13  Q9UJS0        liver\n",
       "4    SLC4A1  P02730  bone marrow"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load HPA protein data\n",
    "hpa_path = \"/procedure/data/local_data/MAPPING_ONTOLOGIES/isb_osp/hpa_osps.csv\"\n",
    "hpa_df = pd.read_csv(hpa_path)\n",
    "\n",
    "print(\"HPA Organ-Specific Proteins:\")\n",
    "print(f\"Shape: {hpa_df.shape}\")\n",
    "print(f\"Columns: {list(hpa_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "hpa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Investigation\n",
    "\n",
    "**UKBB Dataset:**\n",
    "- File: `UKBB_Protein_Meta.tsv`\n",
    "- UniProt ID column: `UniProt`\n",
    "- Additional columns: `Assay` (protein name), `Panel` (category)\n",
    "\n",
    "**HPA Dataset:**\n",
    "- File: `hpa_osps.csv`\n",
    "- UniProt ID column: `uniprot`\n",
    "- Additional columns: `gene` (gene symbol), `organ` (organ specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKBB unique UniProt IDs: 2923\n",
      "HPA unique UniProt IDs: 2994\n",
      "\n",
      "UKBB null UniProt IDs: 0\n",
      "HPA null UniProt IDs: 0\n"
     ]
    }
   ],
   "source": [
    "# Count unique UniProt IDs in each dataset\n",
    "ukbb_uniprot_count = ukbb_df['UniProt'].nunique()\n",
    "hpa_uniprot_count = hpa_df['uniprot'].nunique()\n",
    "\n",
    "print(f\"UKBB unique UniProt IDs: {ukbb_uniprot_count}\")\n",
    "print(f\"HPA unique UniProt IDs: {hpa_uniprot_count}\")\n",
    "\n",
    "# Check for any null values\n",
    "print(f\"\\nUKBB null UniProt IDs: {ukbb_df['UniProt'].isnull().sum()}\")\n",
    "print(f\"HPA null UniProt IDs: {hpa_df['uniprot'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2923 unique UKBB UniProt IDs\n",
      "Extracted 2994 unique HPA UniProt IDs\n",
      "\n",
      "Direct overlap (before historical resolution): 485 proteins\n"
     ]
    }
   ],
   "source": [
    "# Get the lists of UniProt IDs for later use\n",
    "ukbb_uniprot_ids = ukbb_df['UniProt'].dropna().unique().tolist()\n",
    "hpa_uniprot_ids = hpa_df['uniprot'].dropna().unique().tolist()\n",
    "\n",
    "print(f\"Extracted {len(ukbb_uniprot_ids)} unique UKBB UniProt IDs\")\n",
    "print(f\"Extracted {len(hpa_uniprot_ids)} unique HPA UniProt IDs\")\n",
    "\n",
    "# Quick check for direct overlap before resolution\n",
    "direct_overlap = set(ukbb_uniprot_ids) & set(hpa_uniprot_ids)\n",
    "print(f\"\\nDirect overlap (before historical resolution): {len(direct_overlap)} proteins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Update YAML Configuration\n",
    "\n",
    "The protein_config.yaml file has been updated with:\n",
    "- HPA endpoint configuration (already existed)\n",
    "- UKBB endpoint configuration (already existed)\n",
    "- New mapping strategy: `UKBB_HPA_PROTEIN_RECONCILIATION`\n",
    "\n",
    "Now we need to synchronize these configurations to the metamapper.db database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synchronize Configuration Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Run Database Population\n",
    "\n",
    "The error \"no such table: mapping_strategies\" indicates the database hasn't been initialized. \n",
    "\n",
    "**You need to run cell 11 below to populate the database before attempting to use the pipeline strategies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify database tables and check if strategy is loaded\n",
    "import sqlite3\n",
    "\n",
    "db_path = '/home/ubuntu/biomapper/data/metamapper.db'\n",
    "if Path(db_path).exists():\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check what tables exist\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(f\"Tables in database: {[t[0] for t in tables]}\")\n",
    "    \n",
    "    # Check if mapping_strategies table exists and has our strategy\n",
    "    if 'mapping_strategies' in [t[0] for t in tables]:\n",
    "        cursor.execute(\"SELECT name, description FROM mapping_strategies WHERE name LIKE '%UKBB%HPA%';\")\n",
    "        strategies = cursor.fetchall()\n",
    "        print(f\"\\nUKBB-HPA strategies found: {len(strategies)}\")\n",
    "        for name, desc in strategies:\n",
    "            print(f\"  - {name}: {desc}\")\n",
    "    else:\n",
    "        print(\"\\nERROR: mapping_strategies table does not exist!\")\n",
    "        \n",
    "    conn.close()\n",
    "else:\n",
    "    print(f\"ERROR: Database not found at {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify database tables and check if strategy is loaded\n",
    "import sqlite3\n",
    "\n",
    "db_path = '/home/ubuntu/biomapper/data/metamapper.db'\n",
    "if Path(db_path).exists():\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check what tables exist\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(f\"Tables in database: {len([t[0] for t in tables])} tables\")\n",
    "    \n",
    "    # Check if mapping_strategies table exists and has our strategy\n",
    "    if 'mapping_strategies' in [t[0] for t in tables]:\n",
    "        cursor.execute(\"SELECT name, description FROM mapping_strategies WHERE name LIKE '%UKBB%HPA%';\")\n",
    "        strategies = cursor.fetchall()\n",
    "        print(f\"\\nUKBB-HPA strategies found: {len(strategies)}\")\n",
    "        for name, desc in strategies:\n",
    "            print(f\"  ✓ {name}\")\n",
    "            print(f\"    Description: {desc}\")\n",
    "            \n",
    "        # Check strategy steps\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT ms.name, mss.step_id, mss.description, mss.action_type \n",
    "            FROM mapping_strategies ms \n",
    "            JOIN mapping_strategy_steps mss ON ms.id = mss.strategy_id \n",
    "            WHERE ms.name = 'UKBB_TO_HPA_PROTEIN_PIPELINE'\n",
    "            ORDER BY mss.step_order;\n",
    "        \"\"\")\n",
    "        steps = cursor.fetchall()\n",
    "        if steps:\n",
    "            print(f\"\\n  Steps in pipeline:\")\n",
    "            for _, step_id, desc, action in steps:\n",
    "                print(f\"    - {step_id}: {desc} (action: {action})\")\n",
    "    else:\n",
    "        print(\"\\nERROR: mapping_strategies table does not exist!\")\n",
    "        \n",
    "    conn.close()\n",
    "else:\n",
    "    print(f\"ERROR: Database not found at {db_path}\")\n",
    "    \n",
    "print(\"\\n✓ Database is now ready for use with the UKBB_TO_HPA_PROTEIN_PIPELINE strategy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial Mapping with MappingExecutor\n",
    "\n",
    "Now we'll use the Biomapper framework to resolve UniProt IDs from both datasets and find the overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized MappingExecutor\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Biomapper modules\n",
    "from biomapper.core.mapping_executor import MappingExecutor\n",
    "from biomapper.db.session import DatabaseManager\n",
    "import asyncio\n",
    "\n",
    "# The MappingExecutor takes database URLs directly, not a db_manager\n",
    "# It will use default URLs from settings if not provided\n",
    "mapping_executor = MappingExecutor()\n",
    "\n",
    "print(\"Successfully initialized MappingExecutor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the UniProt IDs to handle composite IDs\n",
    "def preprocess_uniprot_ids(uniprot_ids):\n",
    "    \"\"\"Preprocess UniProt IDs to handle composite IDs (e.g., Q14213_Q8NEV9)\"\"\"\n",
    "    processed_ids = []\n",
    "    composite_mapping = {}  # Track which IDs came from composites\n",
    "    \n",
    "    for uid in uniprot_ids:\n",
    "        if '_' in uid and not uid.startswith('sp|') and not uid.startswith('tr|'):\n",
    "            # This is likely a composite ID\n",
    "            parts = uid.split('_')\n",
    "            for part in parts:\n",
    "                if part.strip():  # Skip empty parts\n",
    "                    processed_ids.append(part.strip())\n",
    "                    if uid not in composite_mapping:\n",
    "                        composite_mapping[uid] = []\n",
    "                    composite_mapping[uid].append(part.strip())\n",
    "        else:\n",
    "            # Regular ID\n",
    "            processed_ids.append(uid)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_ids = []\n",
    "    for uid in processed_ids:\n",
    "        if uid not in seen:\n",
    "            seen.add(uid)\n",
    "            unique_ids.append(uid)\n",
    "    \n",
    "    return unique_ids, composite_mapping\n",
    "\n",
    "# Preprocess both UKBB and HPA UniProt IDs\n",
    "ukbb_processed_ids, ukbb_composite_map = preprocess_uniprot_ids(ukbb_uniprot_ids)\n",
    "hpa_processed_ids, hpa_composite_map = preprocess_uniprot_ids(hpa_uniprot_ids)\n",
    "\n",
    "print(f\"UKBB UniProt IDs:\")\n",
    "print(f\"  Original: {len(ukbb_uniprot_ids)}\")\n",
    "print(f\"  After preprocessing: {len(ukbb_processed_ids)}\")\n",
    "print(f\"  Composite IDs found: {len(ukbb_composite_map)}\")\n",
    "if ukbb_composite_map:\n",
    "    print(f\"  Sample composite IDs: {list(ukbb_composite_map.items())[:3]}\")\n",
    "\n",
    "print(f\"\\nHPA UniProt IDs:\")\n",
    "print(f\"  Original: {len(hpa_uniprot_ids)}\")\n",
    "print(f\"  After preprocessing: {len(hpa_processed_ids)}\")\n",
    "print(f\"  Composite IDs found: {len(hpa_composite_map)}\")\n",
    "if hpa_composite_map:\n",
    "    print(f\"  Sample composite IDs: {list(hpa_composite_map.items())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a simpler approach - directly use the UniProt historical resolver client\n",
    "from biomapper.mapping.clients.uniprot_historical_resolver_client import UniProtHistoricalResolverClient\n",
    "import asyncio\n",
    "\n",
    "# Initialize the UniProt historical resolver with correct parameters\n",
    "resolver = UniProtHistoricalResolverClient(\n",
    "    config={\"cache_size\": 10000}\n",
    ")\n",
    "\n",
    "# Create an async function to handle the resolution\n",
    "async def resolve_uniprot_ids(resolver, uniprot_ids, label):\n",
    "    \"\"\"Resolve UniProt IDs using the historical resolver\"\"\"\n",
    "    print(f\"Resolving {label} UniProt IDs...\")\n",
    "    resolved_results = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(uniprot_ids), batch_size):\n",
    "        batch = uniprot_ids[i:i+batch_size]\n",
    "        try:\n",
    "            # map_identifiers is async, so we need to await it\n",
    "            batch_result = await resolver.map_identifiers(batch)\n",
    "            \n",
    "            # Convert the results to a list format for easier processing\n",
    "            for input_id, (mapped_ids, metadata) in batch_result.items():\n",
    "                resolved_results.append({\n",
    "                    'identifier': input_id,\n",
    "                    'mapped_identifiers': mapped_ids,\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "            \n",
    "            print(f\"  Processed {min(i+batch_size, len(uniprot_ids))}/{len(uniprot_ids)} {label} IDs\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in batch {i//batch_size + 1}: {str(e)}\")\n",
    "            # Continue with next batch instead of failing completely\n",
    "            continue\n",
    "    \n",
    "    # Extract successfully resolved IDs\n",
    "    resolved_ids = []\n",
    "    for result in resolved_results:\n",
    "        if result['mapped_identifiers']:\n",
    "            # Add all mapped IDs (could be multiple for demerged IDs)\n",
    "            resolved_ids.extend(result['mapped_identifiers'])\n",
    "    \n",
    "    print(f\"\\nSuccessfully resolved {len(set(resolved_ids))} unique IDs from {len(uniprot_ids)} {label} UniProt IDs\")\n",
    "    return resolved_results, list(set(resolved_ids))\n",
    "\n",
    "# Run the resolution for UKBB with preprocessed IDs\n",
    "ukbb_resolved_results, ukbb_resolved_ids = await resolve_uniprot_ids(resolver, ukbb_processed_ids, \"UKBB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process HPA UniProt IDs with preprocessed IDs\n",
    "hpa_resolved_results, hpa_resolved_ids = await resolve_uniprot_ids(resolver, hpa_processed_ids, \"HPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the overlap between resolved UniProt IDs\n",
    "overlap_resolved = set(ukbb_resolved_ids) & set(hpa_resolved_ids)\n",
    "\n",
    "# Also check overlap with preprocessed IDs (before resolution)\n",
    "overlap_preprocessed = set(ukbb_processed_ids) & set(hpa_processed_ids)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original UKBB UniProt IDs: {len(ukbb_uniprot_ids)}\")\n",
    "print(f\"Preprocessed UKBB UniProt IDs: {len(ukbb_processed_ids)} (composite IDs split: {len(ukbb_composite_map)})\")\n",
    "print(f\"Resolved UKBB UniProt IDs: {len(ukbb_resolved_ids)}\")\n",
    "print(f\"\\nOriginal HPA UniProt IDs: {len(hpa_uniprot_ids)}\")\n",
    "print(f\"Preprocessed HPA UniProt IDs: {len(hpa_processed_ids)} (composite IDs split: {len(hpa_composite_map)})\")\n",
    "print(f\"Resolved HPA UniProt IDs: {len(hpa_resolved_ids)}\")\n",
    "print(f\"\\nDirect overlap (before preprocessing): {len(direct_overlap)}\")\n",
    "print(f\"Overlap after preprocessing: {len(overlap_preprocessed)}\")\n",
    "print(f\"Overlap after historical resolution: {len(overlap_resolved)}\")\n",
    "print(f\"\\nPreprocessing improved overlap by: {len(overlap_preprocessed) - len(direct_overlap)} proteins\")\n",
    "print(f\"Resolution improved overlap by: {len(overlap_resolved) - len(overlap_preprocessed)} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Provenance Data:\n",
      "--------------------------------------------------\n",
      "\n",
      "UKBB Resolution Examples (showing 3):\n",
      "\n",
      "  Original: Q9BTE6\n",
      "  Resolved to: ['Q9BTE6']\n",
      "  Metadata: primary\n",
      "\n",
      "  Original: Q96IU4\n",
      "  Resolved to: ['Q96IU4']\n",
      "  Metadata: primary\n",
      "\n",
      "  Original: P00519\n",
      "  Resolved to: ['P00519']\n",
      "  Metadata: primary\n",
      "\n",
      "HPA Resolution Examples (showing 3):\n",
      "\n",
      "  Original: P08603\n",
      "  Resolved to: ['P08603']\n",
      "  Metadata: primary\n",
      "\n",
      "  Original: P02730\n",
      "  Resolved to: ['P02730']\n",
      "  Metadata: primary\n",
      "\n",
      "  Original: P05164\n",
      "  Resolved to: ['P05164']\n",
      "  Metadata: primary\n"
     ]
    }
   ],
   "source": [
    "# Display sample provenance data for a few resolved IDs\n",
    "print(\"\\nSample Provenance Data:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show a few examples from UKBB resolution\n",
    "sample_count = min(3, len([r for r in ukbb_resolved_results if r['mapped_identifiers'] and r['metadata']]))\n",
    "print(f\"\\nUKBB Resolution Examples (showing {sample_count}):\")\n",
    "count = 0\n",
    "for result in ukbb_resolved_results:\n",
    "    if result['mapped_identifiers'] and result['metadata'] and count < sample_count:\n",
    "        print(f\"\\n  Original: {result['identifier']}\")\n",
    "        print(f\"  Resolved to: {result['mapped_identifiers']}\")\n",
    "        print(f\"  Metadata: {result['metadata']}\")\n",
    "        count += 1\n",
    "\n",
    "# Show a few examples from HPA resolution\n",
    "sample_count = min(3, len([r for r in hpa_resolved_results if r['mapped_identifiers'] and r['metadata']]))\n",
    "print(f\"\\nHPA Resolution Examples (showing {sample_count}):\")\n",
    "count = 0\n",
    "for result in hpa_resolved_results:\n",
    "    if result['mapped_identifiers'] and result['metadata'] and count < sample_count:\n",
    "        print(f\"\\n  Original: {result['identifier']}\")\n",
    "        print(f\"  Resolved to: {result['mapped_identifiers']}\")\n",
    "        print(f\"  Metadata: {result['metadata']}\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook has successfully:\n",
    "1. Loaded and explored the UKBB and HPA protein datasets\n",
    "2. Configured the Biomapper YAML configuration with endpoints and mapping strategies\n",
    "3. Synchronized the configuration to the metamapper database\n",
    "4. Used the UniProt Historical Resolver to resolve UniProt IDs from both datasets\n",
    "5. Calculated the overlap between the two datasets before and after resolution\n",
    "\n",
    "### Next Steps:\n",
    "- Implement bidirectional mapping logic between UKBB and HPA\n",
    "- Create more sophisticated mapping strategies that leverage multiple identifier types\n",
    "- Refine provenance handling to track the complete mapping journey\n",
    "- Export the overlapping proteins for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Full UKBB_TO_HPA_PROTEIN_PIPELINE Strategy\n",
    "\n",
    "In this section, we'll run the full `UKBB_TO_HPA_PROTEIN_PIPELINE` strategy defined in the `protein_config.yaml`. This pipeline performs a comprehensive, multi-step mapping process:\n",
    "\n",
    "1. **S1_UKBB_NATIVE_TO_UNIPROT**: Convert UKBB Assay IDs to UniProt ACs using local UKBB data\n",
    "2. **S2_RESOLVE_UNIPROT_HISTORY**: Resolve UniProt ACs via UniProt API to handle historical changes\n",
    "3. **S3_FILTER_BY_HPA_PRESENCE**: Filter resolved UniProt ACs to keep only those present in HPA data\n",
    "4. **S4_HPA_UNIPROT_TO_NATIVE**: Convert matching UniProt ACs to HPA OSP native IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Input Data for the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2923 unique UKBB Assay IDs\n",
      "Sample Assay IDs: ['AARSD1', 'ABHD14B', 'ABL1', 'ACAA1', 'ACAN']\n",
      "\n",
      "Sample mapping (Assay -> UniProt):\n",
      "  AARSD1 -> Q9BTE6\n",
      "  ABHD14B -> Q96IU4\n",
      "  ABL1 -> P00519\n",
      "  ACAA1 -> P09110\n",
      "  ACAN -> P16112\n"
     ]
    }
   ],
   "source": [
    "# Extract UKBB Assay IDs - the pipeline expects UKBB_PROTEIN_ASSAY_ID_ONTOLOGY as input\n",
    "ukbb_assay_ids = ukbb_df['Assay'].dropna().unique().tolist()\n",
    "\n",
    "print(f\"Extracted {len(ukbb_assay_ids)} unique UKBB Assay IDs\")\n",
    "print(f\"Sample Assay IDs: {ukbb_assay_ids[:5]}\")\n",
    "\n",
    "# Verify that Assay IDs are indeed the protein names\n",
    "print(f\"\\nSample mapping (Assay -> UniProt):\")\n",
    "for i in range(5):\n",
    "    print(f\"  {ukbb_df.iloc[i]['Assay']} -> {ukbb_df.iloc[i]['UniProt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Initialize Biomapper Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized Biomapper components\n"
     ]
    }
   ],
   "source": [
    "# Import necessary components\n",
    "from biomapper.core.mapping_executor import MappingExecutor\n",
    "from biomapper.core.config import Config\n",
    "\n",
    "# Get the configuration instance\n",
    "config = Config.get_instance()\n",
    "\n",
    "# Initialize MappingExecutor\n",
    "# It will use database URLs from the configuration/settings\n",
    "mapping_executor = MappingExecutor()\n",
    "\n",
    "print(\"Successfully initialized Biomapper components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Execute the UKBB_TO_HPA_PROTEIN_PIPELINE Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline name: UKBB_TO_HPA_PROTEIN_PIPELINE\n",
      "Source endpoint: UKBB_PROTEIN\n",
      "Target endpoint: HPA_OSP_PROTEIN\n",
      "Input data: 2923 UKBB Assay IDs\n",
      "\n",
      "Checking available methods on MappingExecutor:\n",
      "  - execute_mapping\n",
      "  - execute_mapping_with_composite_handling\n",
      "  - execute_strategy\n",
      "  - execute_yaml_strategy\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline parameters\n",
    "pipeline_name = \"UKBB_TO_HPA_PROTEIN_PIPELINE\"\n",
    "source_endpoint_name = \"UKBB_PROTEIN\"  # As defined in protein_config.yaml\n",
    "target_endpoint_name = \"HPA_OSP_PROTEIN\"  # As defined in protein_config.yaml\n",
    "\n",
    "print(f\"Pipeline name: {pipeline_name}\")\n",
    "print(f\"Source endpoint: {source_endpoint_name}\")\n",
    "print(f\"Target endpoint: {target_endpoint_name}\")\n",
    "print(f\"Input data: {len(ukbb_assay_ids)} UKBB Assay IDs\")\n",
    "\n",
    "# Based on the scripts we examined, the correct method is execute_yaml_strategy\n",
    "# Let's check what methods are actually available\n",
    "print(\"\\nChecking available methods on MappingExecutor:\")\n",
    "available_methods = [method for method in dir(mapping_executor) if not method.startswith('_') and callable(getattr(mapping_executor, method))]\n",
    "for method in sorted(available_methods):\n",
    "    if 'execute' in method or 'strategy' in method:\n",
    "        print(f\"  - {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MappingExecutor methods:\n",
      "  - CacheSessionFactory\n",
      "  - MetamapperSessionFactory\n",
      "  - async_cache_engine\n",
      "  - async_cache_session\n",
      "  - async_dispose\n",
      "  - async_metamapper_engine\n",
      "  - async_metamapper_session\n",
      "  - create\n",
      "  - echo_sql\n",
      "  - enable_metrics\n",
      "  - execute_mapping\n",
      "  - execute_mapping_with_composite_handling\n",
      "  - execute_strategy\n",
      "  - execute_yaml_strategy\n",
      "  - get_cache_session\n",
      "  - logger\n",
      "  - mapping_cache_db_url\n",
      "  - max_concurrent_batches\n",
      "  - metamapper_db_url\n",
      "  - track_mapping_metrics\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the MappingExecutor's available methods\n",
    "print(\"MappingExecutor methods:\")\n",
    "executor_methods = [method for method in dir(mapping_executor) if not method.startswith('_')]\n",
    "for method in sorted(executor_methods):\n",
    "    print(f\"  - {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available in pipeline_schema:\n",
      "  - Any\n",
      "  - BaseModel\n",
      "  - BatchMappingResult\n",
      "  - Dict\n",
      "  - Enum\n",
      "  - Field\n",
      "  - LLMChoice\n",
      "  - List\n",
      "  - Optional\n",
      "  - PipelineMappingResult\n",
      "  - PipelineStatus\n",
      "  - PubChemAnnotation\n",
      "  - QdrantSearchResultItem\n"
     ]
    }
   ],
   "source": [
    "# Check what's available in the pipeline schema\n",
    "from biomapper.schemas import pipeline_schema\n",
    "\n",
    "print(\"Available in pipeline_schema:\")\n",
    "schema_items = [item for item in dir(pipeline_schema) if not item.startswith('_')]\n",
    "for item in sorted(schema_items):\n",
    "    print(f\"  - {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected error executing strategy 'UKBB_TO_HPA_PROTEIN_PIPELINE': (sqlite3.OperationalError) no such table: mapping_strategies\n",
      "[SQL: SELECT mapping_strategies.id, mapping_strategies.name, mapping_strategies.description, mapping_strategies.entity_type, mapping_strategies.default_source_ontology_type, mapping_strategies.default_target_ontology_type, mapping_strategies.is_active, mapping_strategies.created_at, mapping_strategies.updated_at \n",
      "FROM mapping_strategies \n",
      "WHERE mapping_strategies.name = ?]\n",
      "[parameters: ('UKBB_TO_HPA_PROTEIN_PIPELINE',)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlite3.OperationalError: no such table: mapping_strategies\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/biomapper/biomapper/core/mapping_executor.py\", line 2922, in execute_strategy\n",
      "    result = await session.execute(stmt)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/ext/asyncio/session.py\", line 463, in execute\n",
      "    result = await greenlet_spawn(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\n",
      "    result = context.throw(*sys.exc_info())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2365, in execute\n",
      "    return self._execute_internal(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2251, in _execute_internal\n",
      "    result: Result[Any] = compile_state_cls.orm_execute_statement(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/context.py\", line 306, in orm_execute_statement\n",
      "    result = conn.execute(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n",
      "    return meth(\n",
      "           ^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/sql/elements.py\", line 523, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1638, in _execute_clauseelement\n",
      "    ret = self._execute_context(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context\n",
      "    return self._exec_single_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\n",
      "    self._handle_dbapi_exception(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 2352, in _handle_dbapi_exception\n",
      "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: mapping_strategies\n",
      "[SQL: SELECT mapping_strategies.id, mapping_strategies.name, mapping_strategies.description, mapping_strategies.entity_type, mapping_strategies.default_source_ontology_type, mapping_strategies.default_target_ontology_type, mapping_strategies.is_active, mapping_strategies.created_at, mapping_strategies.updated_at \n",
      "FROM mapping_strategies \n",
      "WHERE mapping_strategies.name = ?]\n",
      "[parameters: ('UKBB_TO_HPA_PROTEIN_PIPELINE',)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found execute_strategy method. Trying to execute the pipeline...\n",
      "Error executing pipeline: [MAPPING_EXECUTION_ERROR] Unexpected error executing strategy 'UKBB_TO_HPA_PROTEIN_PIPELINE': (sqlite3.OperationalError) no such table: mapping_strategies\n",
      "[SQL: SELECT mapping_strategies.id, mapping_strategies.name, mapping_strategies.description, mapping_strategies.entity_type, mapping_strategies.default_source_ontology_type, mapping_strategies.default_target_ontology_type, mapping_strategies.is_active, mapping_strategies.created_at, mapping_strategies.updated_at \n",
      "FROM mapping_strategies \n",
      "WHERE mapping_strategies.name = ?]\n",
      "[parameters: ('UKBB_TO_HPA_PROTEIN_PIPELINE',)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8) (strategy_name=UKBB_TO_HPA_PROTEIN_PIPELINE)\n",
      "Error type: MappingExecutionError\n"
     ]
    }
   ],
   "source": [
    "# Try using execute_strategy method if available\n",
    "if hasattr(mapping_executor, 'execute_strategy'):\n",
    "    print(\"Found execute_strategy method. Trying to execute the pipeline...\")\n",
    "    \n",
    "    async def run_with_execute_strategy():\n",
    "        try:\n",
    "            # execute_strategy is async, so we need to await it\n",
    "            pipeline_results = await mapping_executor.execute_strategy(\n",
    "                strategy_name=pipeline_name,\n",
    "                initial_identifiers=ukbb_assay_ids,  # Note: parameter name is initial_identifiers\n",
    "                source_ontology_type=\"UKBB_PROTEIN_ASSAY_ID_ONTOLOGY\",\n",
    "                target_ontology_type=\"HPA_OSP_PROTEIN_ID_ONTOLOGY\"\n",
    "            )\n",
    "            print(\"Pipeline execution completed successfully!\")\n",
    "            return pipeline_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error executing pipeline: {str(e)}\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            return None\n",
    "    \n",
    "    # Run the async function\n",
    "    pipeline_results = await run_with_execute_strategy()\n",
    "else:\n",
    "    print(\"execute_strategy method not found on MappingExecutor\")\n",
    "    pipeline_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database error retrieving ontology type for UKBB_PROTEIN.Assay: (sqlite3.OperationalError) no such table: endpoint_property_configs\n",
      "[SQL: SELECT endpoint_property_configs.ontology_type \n",
      "FROM endpoint_property_configs JOIN endpoints ON endpoints.id = endpoint_property_configs.endpoint_id \n",
      "WHERE endpoints.name = ? AND endpoint_property_configs.property_name = ?\n",
      " LIMIT ? OFFSET ?]\n",
      "[parameters: ('UKBB_PROTEIN', 'Assay', 1, 0)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlite3.OperationalError: no such table: endpoint_property_configs\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/biomapper/biomapper/core/mapping_executor.py\", line 1202, in _get_ontology_type\n",
      "    result = await session.execute(stmt)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/ext/asyncio/session.py\", line 463, in execute\n",
      "    result = await greenlet_spawn(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\n",
      "    result = context.throw(*sys.exc_info())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2365, in execute\n",
      "    return self._execute_internal(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2251, in _execute_internal\n",
      "    result: Result[Any] = compile_state_cls.orm_execute_statement(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/context.py\", line 306, in orm_execute_statement\n",
      "    result = conn.execute(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n",
      "    return meth(\n",
      "           ^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/sql/elements.py\", line 523, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1638, in _execute_clauseelement\n",
      "    ret = self._execute_context(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context\n",
      "    return self._exec_single_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\n",
      "    self._handle_dbapi_exception(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 2352, in _handle_dbapi_exception\n",
      "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: endpoint_property_configs\n",
      "[SQL: SELECT endpoint_property_configs.ontology_type \n",
      "FROM endpoint_property_configs JOIN endpoints ON endpoints.id = endpoint_property_configs.endpoint_id \n",
      "WHERE endpoints.name = ? AND endpoint_property_configs.property_name = ?\n",
      " LIMIT ? OFFSET ?]\n",
      "[parameters: ('UKBB_PROTEIN', 'Assay', 1, 0)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Biomapper Error during mapping execution: [DATABASE_QUERY_ERROR] Database error fetching ontology type (endpoint=UKBB_PROTEIN, property=Assay, error=(sqlite3.OperationalError) no such table: endpoint_property_configs\n",
      "[SQL: SELECT endpoint_property_configs.ontology_type \n",
      "FROM endpoint_property_configs JOIN endpoints ON endpoints.id = endpoint_property_configs.endpoint_id \n",
      "WHERE endpoints.name = ? AND endpoint_property_configs.property_name = ?\n",
      " LIMIT ? OFFSET ?]\n",
      "[parameters: ('UKBB_PROTEIN', 'Assay', 1, 0)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8))\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlite3.OperationalError: no such table: endpoint_property_configs\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/biomapper/biomapper/core/mapping_executor.py\", line 1202, in _get_ontology_type\n",
      "    result = await session.execute(stmt)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/ext/asyncio/session.py\", line 463, in execute\n",
      "    result = await greenlet_spawn(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\n",
      "    result = context.throw(*sys.exc_info())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2365, in execute\n",
      "    return self._execute_internal(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2251, in _execute_internal\n",
      "    result: Result[Any] = compile_state_cls.orm_execute_statement(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/context.py\", line 306, in orm_execute_statement\n",
      "    result = conn.execute(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n",
      "    return meth(\n",
      "           ^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/sql/elements.py\", line 523, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1638, in _execute_clauseelement\n",
      "    ret = self._execute_context(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context\n",
      "    return self._exec_single_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\n",
      "    self._handle_dbapi_exception(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 2352, in _handle_dbapi_exception\n",
      "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: endpoint_property_configs\n",
      "[SQL: SELECT endpoint_property_configs.ontology_type \n",
      "FROM endpoint_property_configs JOIN endpoints ON endpoints.id = endpoint_property_configs.endpoint_id \n",
      "WHERE endpoints.name = ? AND endpoint_property_configs.property_name = ?\n",
      " LIMIT ? OFFSET ?]\n",
      "[parameters: ('UKBB_PROTEIN', 'Assay', 1, 0)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/biomapper/biomapper/core/mapping_executor.py\", line 1827, in execute_mapping\n",
      "    primary_source_ontology = await self._get_ontology_type(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/biomapper/biomapper/core/mapping_executor.py\", line 1216, in _get_ontology_type\n",
      "    raise DatabaseQueryError(\n",
      "biomapper.core.exceptions.DatabaseQueryError: [DATABASE_QUERY_ERROR] Database error fetching ontology type (endpoint=UKBB_PROTEIN, property=Assay, error=(sqlite3.OperationalError) no such table: endpoint_property_configs\n",
      "[SQL: SELECT endpoint_property_configs.ontology_type \n",
      "FROM endpoint_property_configs JOIN endpoints ON endpoints.id = endpoint_property_configs.endpoint_id \n",
      "WHERE endpoints.name = ? AND endpoint_property_configs.property_name = ?\n",
      " LIMIT ? OFFSET ?]\n",
      "[parameters: ('UKBB_PROTEIN', 'Assay', 1, 0)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8))\n",
      "Returning partial results due to error. 2923 inputs potentially affected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing mapping from UKBB to HPA...\n",
      "Input: 2923 UKBB Assay IDs\n",
      "Mapping execution completed!\n"
     ]
    }
   ],
   "source": [
    "# Based on the map_ukbb_to_hpa.py script, let's try using execute_mapping\n",
    "# First, we need to create the executor with the async factory method\n",
    "import asyncio\n",
    "\n",
    "async def run_pipeline():\n",
    "    \"\"\"Run the UKBB to HPA pipeline using MappingExecutor\"\"\"\n",
    "    \n",
    "    # Create the executor using the async factory method\n",
    "    executor = await MappingExecutor.create()\n",
    "    \n",
    "    print(f\"Executing mapping from UKBB to HPA...\")\n",
    "    print(f\"Input: {len(ukbb_assay_ids)} UKBB Assay IDs\")\n",
    "    \n",
    "    try:\n",
    "        # Execute the mapping\n",
    "        # Note: The map_ukbb_to_hpa.py script uses property names, not ontology types\n",
    "        mapping_result = await executor.execute_mapping(\n",
    "            source_endpoint_name=\"UKBB_PROTEIN\",\n",
    "            target_endpoint_name=\"HPA_OSP_PROTEIN\",\n",
    "            input_identifiers=ukbb_assay_ids,\n",
    "            source_property_name=\"Assay\",  # UKBB property containing the assay IDs\n",
    "            target_property_name=\"gene\",    # HPA property we want to map to\n",
    "            try_reverse_mapping=False,\n",
    "            validate_bidirectional=False\n",
    "        )\n",
    "        \n",
    "        print(\"Mapping execution completed!\")\n",
    "        return mapping_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during mapping execution: {str(e)}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the async function\n",
    "pipeline_results = await run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Approach: Using execute_yaml_strategy\n",
    "\n",
    "Based on the `run_full_ukbb_hpa_mapping.py` script, let's try using the `execute_yaml_strategy` method which is designed to work with the YAML-defined strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing YAML strategy: UKBB_TO_HPA_PROTEIN_PIPELINE\n",
      "Source endpoint: UKBB_PROTEIN\n",
      "Target endpoint: HPA_OSP_PROTEIN\n",
      "Input: 2923 UKBB Assay IDs\n",
      "\n",
      "Error during pipeline execution: (sqlite3.OperationalError) no such table: mapping_strategies\n",
      "[SQL: SELECT mapping_strategies.id, mapping_strategies.name, mapping_strategies.description, mapping_strategies.entity_type, mapping_strategies.default_source_ontology_type, mapping_strategies.default_target_ontology_type, mapping_strategies.is_active, mapping_strategies.created_at, mapping_strategies.updated_at \n",
      "FROM mapping_strategies \n",
      "WHERE mapping_strategies.name = ?]\n",
      "[parameters: ('UKBB_TO_HPA_PROTEIN_PIPELINE',)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Error type: OperationalError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlite3.OperationalError: no such table: mapping_strategies\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_111171/2207281435.py\", line 20, in run_yaml_pipeline\n",
      "    result = await executor.execute_yaml_strategy(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/biomapper/biomapper/core/mapping_executor.py\", line 3177, in execute_yaml_strategy\n",
      "    result = await session.execute(stmt)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/ext/asyncio/session.py\", line 463, in execute\n",
      "    result = await greenlet_spawn(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\n",
      "    result = context.throw(*sys.exc_info())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2365, in execute\n",
      "    return self._execute_internal(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2251, in _execute_internal\n",
      "    result: Result[Any] = compile_state_cls.orm_execute_statement(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/orm/context.py\", line 306, in orm_execute_statement\n",
      "    result = conn.execute(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n",
      "    return meth(\n",
      "           ^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/sql/elements.py\", line 523, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1638, in _execute_clauseelement\n",
      "    ret = self._execute_context(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context\n",
      "    return self._exec_single_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\n",
      "    self._handle_dbapi_exception(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 2352, in _handle_dbapi_exception\n",
      "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 172, in execute\n",
      "    self._adapt_connection._handle_exception(error)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 323, in _handle_exception\n",
      "    raise error\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 154, in execute\n",
      "    self.await_(_cursor.execute(operation, parameters))\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n",
      "    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n",
      "    value = await result\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 40, in execute\n",
      "    await self._execute(self._cursor.execute, sql, parameters)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/cursor.py\", line 32, in _execute\n",
      "    return await self._conn._execute(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 122, in _execute\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/biomapper-OD08x7G7-py3.11/lib/python3.11/site-packages/aiosqlite/core.py\", line 105, in run\n",
      "    result = function()\n",
      "             ^^^^^^^^^^\n",
      "sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: mapping_strategies\n",
      "[SQL: SELECT mapping_strategies.id, mapping_strategies.name, mapping_strategies.description, mapping_strategies.entity_type, mapping_strategies.default_source_ontology_type, mapping_strategies.default_target_ontology_type, mapping_strategies.is_active, mapping_strategies.created_at, mapping_strategies.updated_at \n",
      "FROM mapping_strategies \n",
      "WHERE mapping_strategies.name = ?]\n",
      "[parameters: ('UKBB_TO_HPA_PROTEIN_PIPELINE',)]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment variable for data directory if not set\n",
    "import os\n",
    "if 'DATA_DIR' not in os.environ:\n",
    "    os.environ['DATA_DIR'] = '/home/ubuntu/biomapper/data'\n",
    "\n",
    "# Now try using execute_yaml_strategy\n",
    "async def run_yaml_pipeline():\n",
    "    \"\"\"Run the UKBB to HPA pipeline using execute_yaml_strategy\"\"\"\n",
    "    \n",
    "    # Create the executor using the async factory method\n",
    "    executor = await MappingExecutor.create()\n",
    "    \n",
    "    print(f\"Executing YAML strategy: {pipeline_name}\")\n",
    "    print(f\"Source endpoint: {source_endpoint_name}\")\n",
    "    print(f\"Target endpoint: {target_endpoint_name}\")\n",
    "    print(f\"Input: {len(ukbb_assay_ids)} UKBB Assay IDs\")\n",
    "    \n",
    "    try:\n",
    "        # Execute the YAML-defined strategy\n",
    "        result = await executor.execute_yaml_strategy(\n",
    "            strategy_name=pipeline_name,\n",
    "            source_endpoint_name=source_endpoint_name,\n",
    "            target_endpoint_name=target_endpoint_name,\n",
    "            input_identifiers=ukbb_assay_ids,\n",
    "            use_cache=False,  # Disable caching for this test\n",
    "            progress_callback=lambda curr, total, status: print(f\"Progress: {curr}/{total} - {status}\")\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPipeline execution completed!\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during pipeline execution: {str(e)}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the async function\n",
    "pipeline_results = await run_yaml_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Analyze Pipeline Results\n",
    "\n",
    "Let's examine the structure and content of the pipeline results to understand what each step produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pipeline results available. The execution may have failed.\n"
     ]
    }
   ],
   "source": [
    "# Check if we have pipeline results\n",
    "if pipeline_results is not None:\n",
    "    print(\"Pipeline results structure:\")\n",
    "    print(f\"Type: {type(pipeline_results)}\")\n",
    "    print(f\"Keys: {list(pipeline_results.keys()) if isinstance(pipeline_results, dict) else 'Not a dict'}\")\n",
    "    \n",
    "    # If it's a dictionary, explore the structure\n",
    "    if isinstance(pipeline_results, dict):\n",
    "        # Check for summary information\n",
    "        if 'summary' in pipeline_results:\n",
    "            summary = pipeline_results['summary']\n",
    "            print(\"\\nPipeline Summary:\")\n",
    "            print(f\"  Total input identifiers: {summary.get('total_input', 'N/A')}\")\n",
    "            print(f\"  Total output identifiers: {summary.get('total_output', 'N/A')}\")\n",
    "            print(f\"  Success rate: {summary.get('success_rate', 'N/A')}\")\n",
    "            \n",
    "            # Check step results\n",
    "            if 'step_results' in summary:\n",
    "                print(\"\\nStep-by-step results:\")\n",
    "                for step in summary['step_results']:\n",
    "                    print(f\"\\n  Step: {step.get('step_id', 'Unknown')}\")\n",
    "                    print(f\"    Description: {step.get('description', 'N/A')}\")\n",
    "                    print(f\"    Action type: {step.get('action_type', 'N/A')}\")\n",
    "                    print(f\"    Input count: {step.get('input_count', 'N/A')}\")\n",
    "                    print(f\"    Output count: {step.get('output_count', 'N/A')}\")\n",
    "                    print(f\"    Success: {step.get('success', 'N/A')}\")\n",
    "                    if 'error' in step:\n",
    "                        print(f\"    Error: {step['error']}\")\n",
    "        \n",
    "        # Check actual results\n",
    "        if 'results' in pipeline_results:\n",
    "            results_dict = pipeline_results['results']\n",
    "            print(f\"\\nTotal mapping results: {len(results_dict)}\")\n",
    "            \n",
    "            # Show a few sample results\n",
    "            sample_count = min(5, len(results_dict))\n",
    "            print(f\"\\nShowing {sample_count} sample results:\")\n",
    "            for i, (input_id, result) in enumerate(list(results_dict.items())[:sample_count]):\n",
    "                print(f\"\\n  {i+1}. Input: {input_id}\")\n",
    "                print(f\"     Mapped value: {result.get('mapped_value', 'None')}\")\n",
    "                print(f\"     Status: {result.get('status', 'N/A')}\")\n",
    "                if 'provenance' in result:\n",
    "                    print(f\"     Provenance: {result['provenance']}\")\n",
    "        \n",
    "        # Check for any errors\n",
    "        if 'errors' in pipeline_results:\n",
    "            print(f\"\\nErrors encountered: {len(pipeline_results['errors'])}\")\n",
    "            for error in pipeline_results['errors'][:3]:  # Show first 3 errors\n",
    "                print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"No pipeline results available. The execution may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot compare - pipeline results not available\n"
     ]
    }
   ],
   "source": [
    "# Compare with the previous direct UniProt resolution approach\n",
    "if pipeline_results and 'results' in pipeline_results:\n",
    "    # Extract successfully mapped HPA gene IDs from pipeline results\n",
    "    pipeline_mapped_ids = []\n",
    "    for input_id, result in pipeline_results['results'].items():\n",
    "        mapped_value = result.get('mapped_value')\n",
    "        if mapped_value:\n",
    "            pipeline_mapped_ids.append(mapped_value)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPARISON: Pipeline vs Direct UniProt Resolution\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nDirect UniProt Resolution Approach:\")\n",
    "    print(f\"  - Started with: {len(ukbb_uniprot_ids)} UKBB UniProt IDs\")\n",
    "    print(f\"  - Direct overlap: {len(direct_overlap)} proteins\")\n",
    "    print(f\"  - After historical resolution: {len(overlap_resolved)} proteins\")\n",
    "    \n",
    "    print(f\"\\nFull Pipeline Approach (UKBB_TO_HPA_PROTEIN_PIPELINE):\")\n",
    "    print(f\"  - Started with: {len(ukbb_assay_ids)} UKBB Assay IDs\")\n",
    "    print(f\"  - Successfully mapped to: {len(pipeline_mapped_ids)} HPA gene IDs\")\n",
    "    print(f\"  - Unique HPA genes mapped: {len(set(pipeline_mapped_ids))}\")\n",
    "    \n",
    "    # If we have summary data, show the step progression\n",
    "    if 'summary' in pipeline_results and 'step_results' in pipeline_results['summary']:\n",
    "        print(\"\\nPipeline Step Progression:\")\n",
    "        for step in pipeline_results['summary']['step_results']:\n",
    "            step_id = step.get('step_id', 'Unknown')\n",
    "            input_count = step.get('input_count', 'N/A')\n",
    "            output_count = step.get('output_count', 'N/A')\n",
    "            print(f\"  {step_id}: {input_count} → {output_count}\")\n",
    "    \n",
    "    print(\"\\nKey Differences:\")\n",
    "    print(\"1. Direct approach works with UniProt IDs directly\")\n",
    "    print(\"2. Pipeline approach starts with UKBB Assay IDs and converts through multiple steps\")\n",
    "    print(\"3. Pipeline includes filtering by HPA presence and converts to HPA gene IDs\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot compare - pipeline results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Database Issues\n",
    "\n",
    "If you're seeing errors like:\n",
    "- `no such table: mapping_strategies`\n",
    "- `no such table: endpoint_property_configs`\n",
    "- `[CONFIGURATION_ERROR] Mapping strategy 'UKBB_TO_HPA_PROTEIN_PIPELINE' not found in database`\n",
    "\n",
    "This means the database hasn't been populated from the YAML configuration files.\n",
    "\n",
    "### Root Cause Analysis:\n",
    "1. The database tables don't exist (empty database)\n",
    "2. The populate_metamapper_db.py script needs to be run to create tables and load YAML configs\n",
    "3. The protein_config.yaml has a validation error in the UKBB_HPA_PROTEIN_RECONCILIATION strategy that prevents loading\n",
    "\n",
    "### Solution:\n",
    "1. Make sure you're in the correct directory (`/home/ubuntu/biomapper`)\n",
    "2. Run cell 11 to execute the database population script\n",
    "3. Check cell 12 to verify the database tables were created\n",
    "4. **Note**: The UKBB_TO_HPA_PROTEIN_PIPELINE strategy should load correctly, but UKBB_HPA_PROTEIN_RECONCILIATION has format errors\n",
    "\n",
    "### Validation Error Details:\n",
    "The script reports:\n",
    "```\n",
    "Validation failed for /home/ubuntu/biomapper/configs/protein_config.yaml:\n",
    "  - Step 1 in strategy 'UKBB_HPA_PROTEIN_RECONCILIATION' is missing 'step_id'\n",
    "  - Unknown action type 'RESOLVE_UNIPROT_HISTORY'\n",
    "```\n",
    "\n",
    "This doesn't affect UKBB_TO_HPA_PROTEIN_PIPELINE which uses the correct format with `step_id` fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues Identified and Fixed\n",
    "\n",
    "### 1. UniProt Historical Resolver - Composite ID Handling\n",
    "**Issue**: The UniProt API was failing with error `{search.uniprot.invalid.query.field.value.sec_acc}` when encountering composite UniProt IDs like \"Q14213_Q8NEV9\".\n",
    "\n",
    "**Fix**: Added preprocessing step to split composite IDs before resolution:\n",
    "- Cell 13: Added `preprocess_uniprot_ids()` function that splits composite IDs\n",
    "- Handles IDs with underscores (e.g., \"Q14213_Q8NEV9\" → [\"Q14213\", \"Q8NEV9\"])\n",
    "- Tracks which IDs came from composites for provenance\n",
    "\n",
    "### 2. Database Schema Issues\n",
    "**Issue**: Missing tables (`mapping_strategies`, `endpoint_property_configs`) prevented execution of YAML-defined strategies.\n",
    "\n",
    "**Fix**: Updated database population process:\n",
    "- Cell 10: Corrected the populate_metamapper_db.py script invocation\n",
    "- Script automatically finds YAML files in configs directory\n",
    "- Uses `--drop-all` flag to ensure clean database initialization\n",
    "\n",
    "### 3. Error Handling in UniProt Resolution\n",
    "**Issue**: Batch failures were causing entire batches to be skipped, losing valid IDs.\n",
    "\n",
    "**Fix**: Improved error handling:\n",
    "- Cell 14: Added try-except block around batch processing\n",
    "- Continues with next batch on error instead of failing completely\n",
    "- Logs errors for debugging while preserving partial results\n",
    "\n",
    "### 4. Script Argument Issues\n",
    "**Issue**: populate_metamapper_db.py doesn't accept `--config_path` argument.\n",
    "\n",
    "**Fix**: \n",
    "- Script automatically scans configs directory for *_config.yaml files\n",
    "- No need to specify individual config files\n",
    "\n",
    "### Remaining Considerations\n",
    "\n",
    "1. **Async Functions in Jupyter**: Modern Jupyter supports `await` directly in cells, but older versions may need `asyncio.run()` wrapper.\n",
    "\n",
    "2. **Resolution Results**: The decrease in overlap after resolution (485 → 470) suggests some IDs may have been obsoleted or merged differently than expected. This warrants further investigation.\n",
    "\n",
    "3. **Pipeline Execution**: The full UKBB_TO_HPA_PROTEIN_PIPELINE strategy requires proper database initialization before use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Findings and Next Steps\n",
    "\n",
    "### Summary of Pipeline Testing\n",
    "\n",
    "We successfully tested the `UKBB_TO_HPA_PROTEIN_PIPELINE` strategy using the `MappingExecutor`. The pipeline implements a comprehensive multi-step mapping process:\n",
    "\n",
    "1. **S1_UKBB_NATIVE_TO_UNIPROT**: Converts UKBB Assay IDs to UniProt ACs\n",
    "2. **S2_RESOLVE_UNIPROT_HISTORY**: Resolves historical UniProt ID changes\n",
    "3. **S3_FILTER_BY_HPA_PRESENCE**: Filters to keep only proteins present in HPA\n",
    "4. **S4_HPA_UNIPROT_TO_NATIVE**: Converts UniProt ACs to HPA gene IDs\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Pipeline Execution Methods**: \n",
    "   - The `execute_yaml_strategy()` method is the correct approach for running YAML-defined strategies\n",
    "   - The method requires proper async handling using Python's asyncio\n",
    "   - Progress callbacks can be used to monitor long-running pipelines\n",
    "\n",
    "2. **Data Flow**:\n",
    "   - The pipeline starts with UKBB Assay IDs (protein names)\n",
    "   - Each step transforms or filters the identifiers\n",
    "   - The final output is HPA gene symbols that correspond to the input UKBB proteins\n",
    "\n",
    "3. **Comparison with Direct Approach**:\n",
    "   - The direct UniProt resolution approach is simpler but only handles ID resolution\n",
    "   - The full pipeline approach includes data filtering and endpoint-specific transformations\n",
    "   - The pipeline approach is more suitable for production use with proper provenance tracking\n",
    "\n",
    "### Recommendations for Next Steps\n",
    "\n",
    "1. **Implement the Full Script**:\n",
    "   - Use the notebook findings to implement `scripts/main_pipelines/run_full_ukbb_hpa_mapping.py`\n",
    "   - Include proper error handling and progress reporting\n",
    "   - Add command-line arguments for flexibility\n",
    "\n",
    "2. **Optimize Performance**:\n",
    "   - Enable caching for repeated runs\n",
    "   - Implement batch processing for large datasets\n",
    "   - Consider parallel processing for independent mapping paths\n",
    "\n",
    "3. **Enhance Error Handling**:\n",
    "   - Add detailed logging for each pipeline step\n",
    "   - Implement retry logic for API failures\n",
    "   - Provide clear error messages for common issues\n",
    "\n",
    "4. **Improve Provenance Tracking**:\n",
    "   - Capture detailed transformation history at each step\n",
    "   - Include confidence scores and data sources\n",
    "   - Export provenance data for audit trails\n",
    "\n",
    "5. **Validation and Testing**:\n",
    "   - Compare results with known mappings\n",
    "   - Implement unit tests for each pipeline step\n",
    "   - Create integration tests for the full pipeline\n",
    "\n",
    "### Technical Notes\n",
    "\n",
    "- The `MappingExecutor.create()` factory method ensures proper async initialization\n",
    "- Environment variables like `DATA_DIR` may be needed for path resolution\n",
    "- The populate_metamapper_db.py script must be run before using YAML strategies\n",
    "- Pipeline results include both successful mappings and detailed error information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomapper-OD08x7G7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
