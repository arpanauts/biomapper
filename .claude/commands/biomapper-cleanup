#!/usr/bin/env python3
"""
Enhanced Biomapper Cleanup - Script Artifacts Edition
Extends the existing biomapper-cleanup command to handle one-time scripts and artifacts.

Usage: /biomapper-cleanup --include-scripts [other options]
"""

import os
import re
import ast
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import fnmatch


@dataclass
class ScriptArtifact:
    """Represents a script artifact that may need cleanup."""
    path: Path
    artifact_type: str  # 'one_time_script', 'test_script', 'debug_script', 'log_file', 'temp_file'
    confidence: int     # 0-100
    reasons: List[str]
    last_modified: Optional[datetime] = None
    size_bytes: int = 0
    suggested_action: str = 'archive'  # 'delete', 'archive', 'keep'


class ScriptArtifactAnalyzer:
    """Analyzes Python scripts and other files to identify cleanup candidates."""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.current_time = datetime.now()
        
    def analyze_root_directory(self) -> List[ScriptArtifact]:
        """Analyze root directory for script artifacts."""
        artifacts = []
        
        # Only analyze root directory files
        for item in self.project_root.iterdir():
            if item.is_file():
                artifact = self._analyze_file(item)
                if artifact:
                    artifacts.append(artifact)
        
        return artifacts
    
    def _analyze_file(self, file_path: Path) -> Optional[ScriptArtifact]:
        """Analyze a single file to determine if it's a cleanup candidate."""
        if not file_path.is_file():
            return None
        
        # Get file stats
        try:
            stat = file_path.stat()
            last_modified = datetime.fromtimestamp(stat.st_mtime)
            size_bytes = stat.st_size
        except OSError:
            return None
        
        # Analyze by file type and patterns
        artifact = None
        
        if file_path.suffix == '.py':
            artifact = self._analyze_python_script(file_path, last_modified, size_bytes)
        elif file_path.suffix == '.log':
            artifact = self._analyze_log_file(file_path, last_modified, size_bytes)
        elif file_path.name.startswith('test_') and file_path.suffix in ['.txt', '.json', '.csv']:
            artifact = self._analyze_test_output_file(file_path, last_modified, size_bytes)
        elif file_path.suffix in ['.tmp', '.temp', '.bak']:
            artifact = self._analyze_temp_file(file_path, last_modified, size_bytes)
        
        return artifact
    
    def _analyze_python_script(self, file_path: Path, last_modified: datetime, size_bytes: int) -> Optional[ScriptArtifact]:
        """Analyze Python script for cleanup candidacy."""
        reasons = []
        confidence = 0
        artifact_type = 'unknown'
        suggested_action = 'keep'
        
        filename = file_path.name.lower()
        
        # One-time script patterns
        one_time_patterns = [
            'quick_', 'test_', 'debug_', 'temp_', 'tmp_',
            'run_', 'upload_', 'verify_', 'time_',
            'extract_', 'process_', 'analyze_', 'check_',
            'fix_', 'migrate_', 'convert_', 'export_',
            'import_', 'sync_', 'backup_', 'restore_'
        ]
        
        # Check filename patterns
        for pattern in one_time_patterns:
            if filename.startswith(pattern):
                artifact_type = 'one_time_script'
                confidence += 40
                reasons.append(f'Filename pattern suggests one-time script: {pattern}')
                break
        
        # Production/testing script patterns
        production_patterns = [
            'run_production_', 'run_v3_', 'run_direct_', 'run_with_',
            'quick_test', 'quick_timing', 'time_data_processing'
        ]
        
        for pattern in production_patterns:
            if pattern in filename:
                artifact_type = 'one_time_script'
                confidence += 50
                reasons.append(f'Production/testing script pattern: {pattern}')
                suggested_action = 'archive'
                break
        
        # Analyze file content for additional clues
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Look for one-time script indicators
            one_time_indicators = [
                'if __name__ == "__main__":',
                '# One-time script',
                '# Quick test',
                '# Debug script',
                '# TODO: Remove after',
                '# TEMP:',
                '# FIXME: Delete',
                'print("Debug:',
                'print("Testing:',
                'import pdb',
                'pdb.set_trace',
                'input("Press Enter"',
                'time.sleep(',
            ]
            
            for indicator in one_time_indicators:
                if indicator in content:
                    confidence += 15
                    reasons.append(f'Contains one-time indicator: {indicator}')
            
            # Check for hardcoded paths or values (indicates one-time use)
            hardcoded_patterns = [
                r'/home/ubuntu/biomapper/',
                r'kg2_10_2c',
                r'v3\.0',
                r'production_complete',
                r'direct_no_api',
            ]
            
            for pattern in hardcoded_patterns:
                if re.search(pattern, content):
                    confidence += 20
                    reasons.append(f'Contains hardcoded values: {pattern}')
            
            # Check if it's a simple wrapper or runner script
            if content.count('\n') < 50 and 'subprocess' in content:
                confidence += 25
                reasons.append('Simple runner/wrapper script (likely one-time)')
            
            # Check for imports that suggest testing/debugging
            debug_imports = ['pdb', 'cProfile', 'timeit', 'memory_profiler']
            for imp in debug_imports:
                if f'import {imp}' in content or f'from {imp}' in content:
                    confidence += 30
                    reasons.append(f'Imports debugging/profiling module: {imp}')
        
        except Exception:
            # Can't read file, reduce confidence
            confidence = max(0, confidence - 20)
            reasons.append('Could not analyze file content')
        
        # Age-based analysis
        days_old = (self.current_time - last_modified).days
        if days_old > 90:  # Older than 3 months
            confidence += 20
            reasons.append(f'File is {days_old} days old (likely stale)')
            if artifact_type == 'one_time_script':
                suggested_action = 'archive'
        elif days_old > 30:  # Older than 1 month
            confidence += 10
            reasons.append(f'File is {days_old} days old')
        
        # Size-based analysis (very small scripts are often one-time)
        if size_bytes < 1000:  # Less than 1KB
            confidence += 15
            reasons.append(f'Very small script ({size_bytes} bytes, likely simple one-time script)')
        
        # Only consider it an artifact if we have some confidence
        if confidence >= 30:
            if artifact_type == 'unknown':
                artifact_type = 'one_time_script'
            
            # Adjust suggested action based on confidence
            if confidence >= 80:
                suggested_action = 'archive'
            elif confidence >= 60:
                suggested_action = 'archive'
            else:
                suggested_action = 'review'
            
            return ScriptArtifact(
                path=file_path,
                artifact_type=artifact_type,
                confidence=confidence,
                reasons=reasons,
                last_modified=last_modified,
                size_bytes=size_bytes,
                suggested_action=suggested_action
            )
        
        return None
    
    def _analyze_log_file(self, file_path: Path, last_modified: datetime, size_bytes: int) -> Optional[ScriptArtifact]:
        """Analyze log files for cleanup."""
        reasons = []
        confidence = 70  # Log files are generally cleanup candidates
        
        filename = file_path.name.lower()
        
        # Log file patterns
        if filename.endswith('.log'):
            reasons.append('Log file extension')
            
        # Age-based confidence
        days_old = (self.current_time - last_modified).days
        if days_old > 30:
            confidence += 20
            reasons.append(f'Log file is {days_old} days old')
        
        # Version-specific logs (like v3.0_run.log)
        if re.search(r'v\d+\.\d+', filename):
            confidence += 25
            reasons.append('Version-specific log file')
        
        return ScriptArtifact(
            path=file_path,
            artifact_type='log_file',
            confidence=confidence,
            reasons=reasons,
            last_modified=last_modified,
            size_bytes=size_bytes,
            suggested_action='archive' if days_old > 30 else 'review'
        )
    
    def _analyze_test_output_file(self, file_path: Path, last_modified: datetime, size_bytes: int) -> Optional[ScriptArtifact]:
        """Analyze test output files."""
        reasons = ['Test output file pattern']
        confidence = 75
        
        days_old = (self.current_time - last_modified).days
        if days_old > 7:  # Test outputs older than a week
            confidence += 15
            reasons.append(f'Test output is {days_old} days old')
        
        return ScriptArtifact(
            path=file_path,
            artifact_type='test_output',
            confidence=confidence,
            reasons=reasons,
            last_modified=last_modified,
            size_bytes=size_bytes,
            suggested_action='delete' if days_old > 30 else 'archive'
        )
    
    def _analyze_temp_file(self, file_path: Path, last_modified: datetime, size_bytes: int) -> Optional[ScriptArtifact]:
        """Analyze temporary files."""
        reasons = ['Temporary file extension']
        confidence = 90
        
        return ScriptArtifact(
            path=file_path,
            artifact_type='temp_file',
            confidence=confidence,
            reasons=reasons,
            last_modified=last_modified,
            size_bytes=size_bytes,
            suggested_action='delete'
        )


class ScriptCleanupExecutor:
    """Executes cleanup actions on identified script artifacts."""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        
    def execute_cleanup(self, artifacts: List[ScriptArtifact], dry_run: bool = True) -> Dict:
        """Execute cleanup actions on artifacts."""
        results = {
            'total_artifacts': len(artifacts),
            'actions_taken': 0,
            'archived': 0,
            'deleted': 0,
            'kept': 0,
            'errors': [],
            'summary': []
        }
        
        # Create archive directory if needed
        archive_dir = self.project_root / 'archive' / 'one_time_scripts'
        if not dry_run:
            archive_dir.mkdir(parents=True, exist_ok=True)
        
        for artifact in artifacts:
            try:
                if artifact.suggested_action == 'delete':
                    if not dry_run:
                        artifact.path.unlink()
                    results['deleted'] += 1
                    results['actions_taken'] += 1
                    results['summary'].append(f"Deleted: {artifact.path.name}")
                    
                elif artifact.suggested_action == 'archive':
                    if not dry_run:
                        # Move to archive with timestamp
                        timestamp = datetime.now().strftime('%Y%m%d')
                        new_name = f"{timestamp}_{artifact.path.name}"
                        new_path = archive_dir / new_name
                        artifact.path.rename(new_path)
                    results['archived'] += 1
                    results['actions_taken'] += 1
                    results['summary'].append(f"Archived: {artifact.path.name}")
                    
                else:  # keep or review
                    results['kept'] += 1
                    results['summary'].append(f"Kept: {artifact.path.name} (needs review)")
                    
            except Exception as e:
                results['errors'].append(f"Error processing {artifact.path.name}: {e}")
        
        return results


def analyze_your_script_list():
    """Analyze the specific scripts you mentioned."""
    script_list = [
        'quick_test.py',
        'quick_timing_test.py', 
        'run_direct_no_api.py',
        'run_production_complete.py',
        'run_production_direct.py',
        'run_production_via_api.py',
        'run_production_with_gdrive.py',
        'run_production_with_monitoring.py',
        'run_v3_pipeline_direct.py',
        'run_with_kg2_10_2c.py',
        'test_output.log',
        'time_data_processing.py',
        'upload_kg2_10_2c_results.py',
        'upload_to_gdrive.py',
        'v3.0_run.log',
        'verify_extraction_logic.py',
        'verify_match_stats.py'
    ]
    
    print("📊 Analysis of Your Root Directory Scripts:")
    print("=" * 50)
    
    # Categorize the scripts
    categories = {
        'Production Runners (Archive)': [
            'run_production_complete.py',
            'run_production_direct.py', 
            'run_production_via_api.py',
            'run_production_with_gdrive.py',
            'run_production_with_monitoring.py',
            'run_v3_pipeline_direct.py',
            'run_direct_no_api.py'
        ],
        'Quick Tests (Archive)': [
            'quick_test.py',
            'quick_timing_test.py',
            'time_data_processing.py'
        ],
        'Verification Scripts (Archive)': [
            'verify_extraction_logic.py',
            'verify_match_stats.py'
        ],
        'Upload Scripts (Archive)': [
            'upload_kg2_10_2c_results.py',
            'upload_to_gdrive.py',
            'run_with_kg2_10_2c.py'
        ],
        'Log Files (Delete)': [
            'test_output.log',
            'v3.0_run.log'
        ]
    }
    
    total_cleanup = 0
    for category, files in categories.items():
        action = "ARCHIVE" if "Archive" in category else "DELETE"
        print(f"\n{category}:")
        for file in files:
            if file in script_list:
                print(f"   📄 {file} → {action}")
                total_cleanup += 1
    
    print(f"\n📈 Summary:")
    print(f"   Total files identified: {total_cleanup}")
    print(f"   Archive candidates: {sum(len(files) for cat, files in categories.items() if 'Archive' in cat)}")
    print(f"   Delete candidates: {sum(len(files) for cat, files in categories.items() if 'Delete' in cat)}")
    print(f"   Root directory reduction: ~{total_cleanup} files")
    
    return categories


def main():
    """Enhanced main function with script cleanup capabilities."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Enhanced biomapper cleanup with script artifact detection",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  /biomapper-cleanup --include-scripts                    # Include script analysis
  /biomapper-cleanup --scripts-only                       # Only analyze scripts
  /biomapper-cleanup --include-scripts --dry-run=false    # Actually clean scripts
  /biomapper-cleanup --analyze-list                       # Analyze your specific list
        """
    )
    
    parser.add_argument('--include-scripts', action='store_true',
                       help='Include script artifact analysis')
    parser.add_argument('--scripts-only', action='store_true', 
                       help='Only analyze script artifacts (skip regular cleanup)')
    parser.add_argument('--analyze-list', action='store_true',
                       help='Analyze the specific script list you provided')
    parser.add_argument('--dry-run', type=bool, default=True,
                       help='Show what would be cleaned without making changes')
    parser.add_argument('--confidence', type=int, default=70,
                       help='Minimum confidence threshold for script cleanup (0-100)')
    
    args = parser.parse_args()
    
    if args.analyze_list:
        analyze_your_script_list()
        return
    
    project_root = Path.cwd()
    
    print("🧹 Enhanced Biomapper Cleanup - Script Artifacts Edition")
    print("=" * 60)
    print(f"Project root: {project_root}")
    print(f"Mode: {'DRY RUN' if args.dry_run else 'CLEANUP'}")
    print()
    
    if args.include_scripts or args.scripts_only:
        print("📊 Analyzing script artifacts in root directory...")
        
        analyzer = ScriptArtifactAnalyzer(project_root)
        artifacts = analyzer.analyze_root_directory()
        
        # Filter by confidence
        high_confidence_artifacts = [a for a in artifacts if a.confidence >= args.confidence]
        
        print(f"   Found {len(artifacts)} potential script artifacts")
        print(f"   High confidence (>={args.confidence}%): {len(high_confidence_artifacts)}")
        print()
        
        if high_confidence_artifacts:
            print("📋 Script Artifact Analysis:")
            print("-" * 40)
            
            # Group by suggested action
            actions = {}
            for artifact in high_confidence_artifacts:
                action = artifact.suggested_action
                if action not in actions:
                    actions[action] = []
                actions[action].append(artifact)
            
            for action, artifacts_list in actions.items():
                print(f"\n{action.upper()} ({len(artifacts_list)} files):")
                for artifact in artifacts_list:
                    age = (datetime.now() - artifact.last_modified).days if artifact.last_modified else 0
                    print(f"   📄 {artifact.path.name} [{artifact.confidence}%] ({age} days old)")
                    print(f"      Type: {artifact.artifact_type}")
                    for reason in artifact.reasons[:2]:  # Show first 2 reasons
                        print(f"      • {reason}")
            
            # Execute cleanup
            if not args.dry_run:
                print(f"\n🔧 Executing cleanup actions...")
                executor = ScriptCleanupExecutor(project_root)
                results = executor.execute_cleanup(high_confidence_artifacts, dry_run=False)
                
                print(f"✅ Cleanup completed:")
                print(f"   Archived: {results['archived']} files")
                print(f"   Deleted: {results['deleted']} files") 
                print(f"   Kept: {results['kept']} files")
                
                if results['errors']:
                    print(f"❌ Errors: {len(results['errors'])}")
                    for error in results['errors']:
                        print(f"   • {error}")
            else:
                print(f"\n💡 To execute cleanup: /biomapper-cleanup --include-scripts --dry-run=false")
        else:
            print("✅ No high-confidence script artifacts found for cleanup")
    
    if not args.scripts_only:
        print("\n" + "="*60)
        print("Running standard biomapper cleanup...")
        # Here you would call your existing biomapper-cleanup logic
        print("(Standard cleanup logic would run here)")


if __name__ == "__main__":
    main()