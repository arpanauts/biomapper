import asyncio
import enum
import importlib
import json
from typing import List, Dict, Any, Optional, Tuple, Set, Union, Type
from datetime import datetime, timezone, timedelta
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, selectinload, joinedload
from sqlalchemy.future import select
from sqlalchemy import func, update
from sqlalchemy import select
from sqlalchemy.exc import SQLAlchemyError, IntegrityError, DBAPIError
from biomapper.core.exceptions import (
    BiomapperError,
    NoPathFoundError,
    ClientError,
    ConfigurationError,
    CacheError,
    MappingExecutionError,
    ClientExecutionError,
    ClientInitializationError,
    CacheTransactionError,
    CacheRetrievalError,
    CacheStorageError,
    ErrorCode,
    DatabaseQueryError, # Import DatabaseQueryError
)

# Import models for metamapper DB
from ..db.models import (
    Endpoint,
    EndpointPropertyConfig,
    PropertyExtractionConfig,
    MappingPath,
    MappingPathStep,
    MappingResource,
    OntologyPreference,
)

# Import models for cache DB
from ..db.cache_models import (
    Base as CacheBase,  # Import the Base for cache tables
    EntityMapping,
    EntityMappingProvenance,
    PathExecutionLog as MappingPathExecutionLog,
    PathExecutionStatus,
    PathLogMappingAssociation,
    MappingSession,  # Add this for session logging
)

# Import our centralized configuration settings
from biomapper.config import settings

from pathlib import Path # Added import

import logging # Re-added import

# Added get_current_utc_time definition
def get_current_utc_time() -> datetime:
    """Return the current time in UTC timezone."""
    return datetime.now(timezone.utc)

class ReversiblePath:
    """Wrapper to allow executing a path in reverse direction."""

    def __init__(self, original_path: MappingPath, is_reverse: bool = False):
        self.original_path = original_path
        self.is_reverse = is_reverse

    @property
    def id(self) -> Optional[int]:
        return self.original_path.id

    @property
    def name(self) -> Optional[str]:
        return (
            f"{self.original_path.name} (Reverse)"
            if self.is_reverse
            else self.original_path.name
        )

    @property
    def priority(self) -> Optional[int]:
        # Reverse paths have slightly lower priority
        original_priority = self.original_path.priority if self.original_path.priority is not None else 100 # Default priority if None
        return original_priority + (5 if self.is_reverse else 0)

    @property
    def steps(self) -> List[MappingPathStep]:
        if not self.is_reverse:
            return self.original_path.steps
        else:
            # Return steps in reverse order
            return sorted(self.original_path.steps, key=lambda s: -(s.step_order or 0))

    def __getattr__(self, name: str) -> Any:
        # Delegate other attributes to the original path
        return getattr(self.original_path, name)


class MappingExecutor:
    """Executes mapping tasks based on configurations in metamapper.db."""

    def __init__(
        self,
        metamapper_db_url: Optional[str] = None,
        mapping_cache_db_url: Optional[str] = None,
        echo_sql: bool = False, # Added parameter to control SQL echoing
    ):
        """
        Initializes the MappingExecutor.

        Args:
            metamapper_db_url: URL for the metamapper database. If None, uses settings.metamapper_db_url.
            mapping_cache_db_url: URL for the mapping cache database. If None, uses settings.cache_db_url.
            echo_sql: If True, echo SQL statements generated by engines. Defaults False.
        """
        self.metamapper_db_url = (
            metamapper_db_url
            if metamapper_db_url is not None
            else settings.metamapper_db_url
        )
        self.mapping_cache_db_url = (
            mapping_cache_db_url
            if mapping_cache_db_url is not None
            else settings.cache_db_url
        )
        self.echo_sql = echo_sql

        self.logger = logging.getLogger(__name__) # Added logger initialization

        self.logger.info(f"Using Metamapper DB URL: {self.metamapper_db_url}")
        self.logger.info(f"Using Mapping Cache DB URL: {self.mapping_cache_db_url}")

        # Ensure directories for file-based DBs exist
        for db_url in [self.metamapper_db_url, self.mapping_cache_db_url]:
            if db_url.startswith("sqlite"):
                try:
                    # Extract path after '///'
                    db_path_str = db_url.split(":///", 1)[1]
                    db_path = Path(db_path_str)
                    db_path.parent.mkdir(parents=True, exist_ok=True)
                    self.logger.debug(f"Ensured directory exists: {db_path.parent}")
                except IndexError:
                    self.logger.error(f"Could not parse file path from SQLite URL: {db_url}")
                except Exception as e:
                    self.logger.error(f"Error ensuring directory for {db_url}: {e}")

        # Setup SQLAlchemy engines and sessions for Metamapper
        meta_async_url = self.metamapper_db_url
        if self.metamapper_db_url.startswith("sqlite:///"):
            meta_async_url = self.metamapper_db_url.replace("sqlite:///", "sqlite+aiosqlite:///")
        self.async_metamapper_engine = create_async_engine(meta_async_url, echo=self.echo_sql)
        self.MetamapperSessionFactory = sessionmaker(
            self.async_metamapper_engine, class_=AsyncSession, expire_on_commit=False
        )
        # Define an async session property for easier access
        self.async_metamapper_session = self.MetamapperSessionFactory

        # Setup SQLAlchemy engines and sessions for Mapping Cache
        cache_async_url = self.mapping_cache_db_url
        if self.mapping_cache_db_url.startswith("sqlite:///"):
            cache_async_url = self.mapping_cache_db_url.replace("sqlite:///", "sqlite+aiosqlite:///")
        self.async_cache_engine = create_async_engine(cache_async_url, echo=self.echo_sql)
        self.CacheSessionFactory = sessionmaker(
            self.async_cache_engine, class_=AsyncSession, expire_on_commit=False
        )
        # Define an async session property for easier access
        self.async_cache_session = self.CacheSessionFactory

    def get_cache_session(self):
        """Get a cache database session."""
        return self.async_cache_session()

    async def _get_path_details(
        self, meta_session: AsyncSession, path_id: int
    ) -> Dict[str, Any]:
        """
        Retrieve details about a mapping path for use in confidence scoring and metadata.

        Args:
            meta_session: SQLAlchemy session for the metamapper database
            path_id: ID of the MappingPath to analyze

        Returns:
            Dict containing path details including:
                - hop_count: Number of steps in the path
                - resource_types: List of resource types used
                - client_identifiers: List of client identifiers used
        """
        # Query to get the path with its steps and resources
        stmt = (
            select(MappingPath)
            .options(
                selectinload(MappingPath.steps).joinedload(
                    MappingPathStep.mapping_resource
                )
            )
            .where(MappingPath.id == path_id)
        )

        result = await meta_session.execute(stmt)
        path = result.scalar_one_or_none()

        if not path:
            self.logger.warning(f"Path ID {path_id} not found in database")
            return {"hop_count": 1, "resource_types": [], "client_identifiers": []}

        # Count the steps as hop count
        steps = sorted(path.steps, key=lambda s: s.step_order)
        hop_count = len(steps)

        # Extract info about resources used
        resource_types = []
        client_identifiers = []

        for step in steps:
            if step.mapping_resource:
                resource_type = getattr(step.mapping_resource, "resource_type", None)
                if resource_type:
                    resource_types.append(resource_type)

                client_id = getattr(step.mapping_resource, "name", None)
                if client_id:
                    client_identifiers.append(client_id)

        return {
            "hop_count": hop_count,
            "resource_types": resource_types,
            "client_identifiers": client_identifiers,
            "path_name": path.name if path else None,
            "path_description": path.description if path else None,
        }

    async def _get_path_details(self, path_id: int) -> Dict[str, Any]:
        """
        Get detailed information about a mapping path including all steps.
        
        Args:
            path_id: The ID of the mapping path
            
        Returns:
            A dictionary with detailed information about the path
        """
        try:
            async with self.get_metamapper_session() as session:
                # Query the path with its steps
                stmt = (select(MappingPath)
                        .where(MappingPath.id == path_id)
                        .options(selectinload(MappingPath.steps)
                                .selectinload(MappingPathStep.mapping_resource)))

                result = await session.execute(stmt)
                path = result.scalar_one_or_none()

                if not path:
                    self.logger.warning(f"Path with ID {path_id} not found in metamapper DB.")
                    return {}

                path_details = {}
                # Add details for each step in the path
                # Sort steps to ensure consistent ordering in details
                sorted_steps = sorted(path.steps, key=lambda s: s.step_order)
                for step in sorted_steps:
                    step_order = step.step_order
                    resource = step.mapping_resource

                    # Create a step entry with relevant details
                    step_key = f"step_{step_order}"
                    path_details[step_key] = {
                        "resource_id": resource.id if resource else None,
                        "resource_name": resource.name if resource else "Unknown",
                        "resource_client": resource.client_class_path if resource else "Unknown",
                        # Use the actual ontology terms stored in the resource
                        "input_ontology": resource.input_ontology_term if resource else "Unknown",
                        "output_ontology": resource.output_ontology_term if resource else "Unknown",
                    }
                
                self.logger.debug(f"Retrieved details for path {path_id}: {path_details}")
                return path_details

        except SQLAlchemyError as e:
            self.logger.warning(f"SQLAlchemyError getting path details for {path_id}: {str(e)}")
            return {} # Return empty dict on DB error, don't block the main operation
        except Exception as e:
            # Catch other potential errors during detail retrieval
            self.logger.warning(f"Unexpected error getting path details for {path_id}: {str(e)}", exc_info=True)
            return {} # Return empty dict on error, don't block the main operation

    async def _cache_results(
        self,
        results_to_cache: Dict[str, Dict[str, Any]],
        path: Union[MappingPath, "ReversiblePath"],
        source_ontology: str,
        target_ontology: str,
        mapping_session_id: Optional[int] = None
    ):
        """
        Store successful mapping results in the cache.
        
        Calculates and populates metadata fields:
        - confidence_score: Based on path length and direction.
        - hop_count: Number of steps in the executed path.
        - mapping_direction: Whether the path was executed in "forward" or "reverse" direction.
        - mapping_path_details: Structured JSON information about the path execution.
        
        Args:
            results_to_cache: Dictionary of source identifiers to mapping results.
            path: MappingPath or ReversiblePath that was executed.
            source_ontology: Source ontology type.
            target_ontology: Target ontology type.
            mapping_session_id: Optional ID of the mapping session.
        
        Returns:
            The ID of the created path execution log entry, or None if no results cached.
        
        Raises:
            CacheStorageError: If there is an error storing the results in the cache.
            CacheTransactionError: If there is an error during the database transaction.
            CacheError: For other unexpected caching errors.
        """
        # Skip if no results to cache
        if not results_to_cache:
            self.logger.debug("No results to cache")
            return None # Return None explicitly

        path_id = path.id
        path_name = path.name
        self.logger.debug(f"Caching results for path ID: {path_id}, Name: {path_name}")

        # Retrieve detailed path information using the helper method
        try:
            path_step_details = await self._get_path_details(path_id)
        except Exception as e:
            # Log error but proceed with caching if possible, using empty details
            self.logger.error(f"Failed to retrieve path details for {path_id} during caching: {e}", exc_info=True)
            path_step_details = {}

        # Determine if this is a reverse path
        is_reversed = getattr(path, "is_reverse", False)
        mapping_direction = "reverse" if is_reversed else "forward"

        # Calculate hop count from path steps if available
        hop_count = len(path.steps) if hasattr(path, "steps") and path.steps else None
        self.logger.debug(f"Path {path_id} - Reversed: {is_reversed}, Hop Count: {hop_count}")

        # Prepare the rich path details JSON structure
        mapping_path_info = {
            "path_id": path_id,
            "path_name": path_name,
            "is_reversed": is_reversed,
            "hop_count": hop_count,
            "steps": path_step_details # Use the retrieved step details
        }
        try:
            # Serialize to JSON
            path_details_json = json.dumps(mapping_path_info)
        except Exception as e:
            self.logger.error(f"Failed to serialize path details for {path_id} to JSON: {e}", exc_info=True)
            path_details_json = json.dumps({"error": "Failed to serialize path details"}) # Fallback JSON

        # Calculate match count accurately
        input_count = len(results_to_cache)
        match_count = sum(
            1 for res in results_to_cache.values()
            if res.get("target_identifiers") and any(res["target_identifiers"])
        )
        self.logger.debug(f"Input Count: {input_count}, Match Count: {match_count}")

        # Create a mapping execution log entry (initially without ID)
        log_entry = MappingPathExecutionLog(
            path_id=path_id,
            source_ontology_type=source_ontology,
            target_ontology_type=target_ontology,
            session_id=mapping_session_id,
            execution_time=get_current_utc_time(), # Use helper method
            status=PathExecutionStatus.SUCCESS,
            input_count=input_count,
            match_count=match_count,
            path_details_json=path_details_json # Store the full JSON here now
        )

        entity_mappings = []
        current_time = get_current_utc_time() # Get time once for consistency

        for source_id, result in results_to_cache.items():
            target_identifiers = result.get("target_identifiers", [])
            # Ensure target_identifiers is always a list
            if not isinstance(target_identifiers, list):
                target_identifiers = [target_identifiers] if target_identifiers is not None else []
            
            # Filter out None values from target identifiers
            valid_target_ids = [tid for tid in target_identifiers if tid is not None]

            if not valid_target_ids:
                self.logger.debug(f"No valid target identifiers found for source {source_id}")
                continue

            # Calculate confidence score using Claude's logic
            confidence_score = result.get("confidence_score") # Allow override from result
            if confidence_score is None:
                if hop_count is not None:
                    if hop_count <= 1:
                        confidence_score = 0.9  # Direct mapping
                    elif hop_count == 2:
                        confidence_score = 0.8  # 2-hop mapping
                    else:
                        # Decrease confidence for longer paths
                        confidence_score = max(0.1, 0.9 - ((hop_count - 1) * 0.1))
                    
                    # Apply penalty for reverse paths
                    if is_reversed:
                        confidence_score = max(0.1, confidence_score - 0.05)
                else:
                    confidence_score = 0.7 # Default if hop_count is somehow None
            
            self.logger.debug(f"Source: {source_id}, Hops: {hop_count}, Reversed: {is_reversed}, Confidence: {confidence_score}")

            # Create entity mapping for each valid target identifier
            for target_id in valid_target_ids:
                entity_mapping = EntityMapping(
                    source_identifier=str(source_id), # Ensure string type
                    target_identifier=str(target_id), # Ensure string type
                    source_ontology_type=source_ontology,
                    target_ontology_type=target_ontology,
                    path_id=path_id,
                    path_name=path_name,
                    creation_time=current_time,
                    # path_execution_log_id will be set after log entry is flushed
                    
                    # Populated Metadata fields:
                    confidence_score=confidence_score,
                    hop_count=hop_count,
                    mapping_direction=mapping_direction,
                    mapping_path_details=path_details_json, # Use the pre-computed JSON
                )
                entity_mappings.append(entity_mapping)

        if not entity_mappings:
            self.logger.warning(f"No valid entity mappings generated for path {path_id}, despite having results to cache. Check input data.")
            # Optionally, still log the execution attempt even if no mappings are created
            try:
                async with self.get_cache_session() as session:
                    log_entry.status = PathExecutionStatus.SUCCESS_NO_MATCH # Indicate success but no mappings
                    log_entry.match_count = 0 # Correct match count
                    session.add(log_entry)
                    await session.commit()
                    self.logger.info(f"Logged execution for path {path_id} with no resulting mappings.")
                    return log_entry.id
            except Exception as e:
                 self.logger.error(f"Failed to log no-match execution for path {path_id}: {e}", exc_info=True)
                 # Decide how to handle this - maybe raise CacheError? For now, just log.
                 return None # Indicate failure to log
            # return None # Indicate nothing was cached

        # Store the log entry and mappings in the cache database
        log_entry_id = None
        try:
            async with self.get_cache_session() as session:
                # Add the log entry first to get its ID
                session.add(log_entry)
                await session.flush() # Generate the ID for log_entry
                log_entry_id = log_entry.id
                self.logger.debug(f"Created MappingPathExecutionLog entry with ID: {log_entry_id}")

                # Update entity mappings with the log entry ID
                for mapping in entity_mappings:
                    mapping.path_execution_log_id = log_entry_id
                
                # Add all entity mappings
                session.add_all(entity_mappings)
                await session.commit() # Commit the transaction

                self.logger.info(f"Successfully cached {len(entity_mappings)} mappings and execution log (ID: {log_entry_id}) for path {path_id}.")
                return log_entry_id # Return the ID of the log entry

        except IntegrityError as e:
            await session.rollback() # Rollback on integrity error (e.g., duplicate)
            self.logger.error(f"IntegrityError during cache storage for path {path_id}: {str(e)}")
            raise CacheStorageError(f"Error storing mapping results in cache: {str(e)}", original_exception=e)
        except SQLAlchemyError as e:
            await session.rollback() # Rollback on other DB errors
            self.logger.error(f"SQLAlchemyError during cache transaction for path {path_id}: {str(e)}", exc_info=True)
            raise CacheTransactionError(f"Error during cache transaction: {str(e)}", original_exception=e)
        except Exception as e:
            # Ensure rollback happens even for unexpected errors within the 'try' block
            # Check if session is active before rolling back
            if 'session' in locals() and session.is_active:
                await session.rollback()
            self.logger.error(f"Unexpected error during caching for path {path_id}: {str(e)}", exc_info=True)
            raise CacheError(f"Unexpected error during caching: {str(e)}", original_exception=e)

    async def _cache_results(
        self,
        results_to_cache: Dict[str, Dict[str, Any]],
        path: Union[MappingPath, "ReversiblePath"],
        source_ontology: str,
        target_ontology: str,
        mapping_session_id: Optional[int] = None
    ):
        """
        Store successful mapping results in the cache.
        
        Calculates and populates metadata fields:
        - confidence_score: Based on path length, client type, or a default value
        - hop_count: Number of steps in the executed path
        - mapping_direction: Whether the path was executed in "forward" or "reverse" direction
        - mapping_path_details: Structured JSON information about the path execution
        
        Args:
            results_to_cache: Dictionary of source identifiers to mapping results.
            path: MappingPath or ReversiblePath that was executed.
            source_ontology: Source ontology type.
            target_ontology: Target ontology type.
            mapping_session_id: Optional ID of the mapping session.
        
        Returns:
            The number of mappings added, or None if no results cached.
        
        Raises:
            CacheStorageError: If there is an error storing the results in the cache.
            CacheTransactionError: If there is an error during the database transaction.
            CacheError: For other unexpected caching errors.
        """
        # Skip if no results to cache
        if not results_to_cache:
            self.logger.debug("No results to cache")
            return None  # Return None explicitly
        
        # Get basic path information
        path_id = path.id if hasattr(path, 'id') else None
        path_name = path.name if hasattr(path, 'name') else "Unknown"
        self.logger.debug(f"Caching results for path ID: {path_id}, Name: {path_name}")
        
        # Retrieve detailed path information using the helper method
        try:
            # Note: Assuming _get_path_details returns a dictionary of step details
            # compatible with the structure needed below.
            path_step_details = await self._get_path_details(path_id)
            if path_step_details is None:
                self.logger.warning(f"_get_path_details returned None for path {path_id}. Using empty details.")
                path_step_details = {}
        except Exception as e:
            # Log error but proceed with caching if possible, using empty details
            self.logger.error(f"Failed to retrieve path details for {path_id} during caching: {e}", exc_info=True)
            path_step_details = {}

        # Determine if this is a reverse path
        # We need a reliable way to determine if 'path' represents a reversed execution.
        # Checking for an attribute like 'is_reverse' added by the calling code is one way.
        # Or, if ReversiblePath is a real class wrapping MappingPath, check isinstance.
        is_reversed = getattr(path, "is_reverse", False) # Assuming an 'is_reverse' flag is set externally
        mapping_direction = "reverse" if is_reversed else "forward"
        
        # Calculate hop count from path steps if available
        # Accessing 'steps' might differ if path is a ReversiblePath wrapper
        actual_path_obj = getattr(path, 'original_path', path) # Check for wrapper pattern
        hop_count = len(actual_path_obj.steps) if hasattr(actual_path_obj, "steps") and actual_path_obj.steps else None
        self.logger.debug(f"Path {path_id} - Reversed: {is_reversed}, Hop Count: {hop_count}")
        
        # Prepare the rich path details JSON structure
        mapping_path_info = {
            "path_id": path_id,
            "path_name": path_name,
            "mapping_direction": mapping_direction,
            "hop_count": hop_count,
            "steps": path_step_details  # Use the retrieved step details
        }
        
        try:
            # Serialize to JSON
            path_details_json = json.dumps(mapping_path_info)
        except Exception as e:
            self.logger.error(f"Failed to serialize path details for path {path_id}: {e}", exc_info=True)
            path_details_json = json.dumps({"error": "Failed to serialize path details", "path_id": path_id})
        
        # Create entity mappings
        mappings_to_add = []
        current_time = get_current_utc_time()  # Get time once for consistency
        
        for source_id, result in results_to_cache.items():
            target_identifiers = result.get("target_identifiers", [])
            # Ensure target_identifiers is always a list
            if not isinstance(target_identifiers, list):
                target_identifiers = [target_identifiers] if target_identifiers is not None else []
            
            # Filter out None values from target identifiers
            valid_target_ids = [tid for tid in target_identifiers if tid is not None]
            
            if not valid_target_ids:
                self.logger.debug(f"No valid target identifiers found for source {source_id}")
                continue
            
            # Calculate confidence score
            confidence_score = result.get("confidence_score")  # Allow override from result
            if confidence_score is None:
                if hop_count is not None:
                    if hop_count <= 1:
                        confidence_score = 0.9  # Direct mapping
                    elif hop_count == 2:
                        confidence_score = 0.8  # 2-hop mapping
                    else:
                        # Decrease confidence for longer paths
                        confidence_score = max(0.1, 0.9 - ((hop_count - 1) * 0.1))
                    
                    # Apply penalty for reverse paths
                    if is_reversed:
                        confidence_score = max(0.1, confidence_score - 0.05)
                else:
                    confidence_score = 0.7  # Default if hop_count is somehow None
            
            self.logger.debug(f"Source: {source_id}, Hops: {hop_count}, Reversed: {is_reversed}, Confidence: {confidence_score}")
            
            # Create entity mapping for each valid target identifier
            for target_id in valid_target_ids:
                entity_mapping = EntityMapping(
                    source_id=str(source_id),  # Ensure string type
                    source_type=source_ontology,
                    target_id=str(target_id),  # Ensure string type
                    target_type=target_ontology,
                    
                    # Populated metadata fields:
                    confidence_score=confidence_score,
                    hop_count=hop_count,
                    mapping_direction=mapping_direction,
                    mapping_path_details=path_details_json,  # Use the pre-computed JSON
                    
                    # Additional fields:
                    last_updated=current_time,
                    # mapping_session_log_id=mapping_session_id, # Consider if needed
                    # path_execution_log_id=path_execution_log_id # Needs to be created/passed
                )
                mappings_to_add.append(entity_mapping)
        
        if not mappings_to_add:
            self.logger.warning(f"No valid entity mappings generated for path {path_id}, despite having results to cache. Check input data.")
            return None  # Indicate nothing was cached
        
        # Store the mappings in the cache database
        try:
            async with self.get_cache_session() as session:
                # Add all entity mappings
                session.add_all(mappings_to_add)
                # TODO: Consider creating and linking PathExecutionLog here if needed
                # path_execution_log = PathExecutionLog(...) 
                # session.add(path_execution_log)
                # await session.flush() # Get ID for path_execution_log
                # for mapping in mappings_to_add: mapping.path_execution_log_id = path_execution_log.id
                await session.commit()  # Commit the transaction
                
                self.logger.info(f"Successfully cached {len(mappings_to_add)} mappings for path {path_id}.")
                return len(mappings_to_add)  # Return the number of mappings added
                
        except SQLAlchemyError as e:
            # Rollback is handled by the async session context manager's __aexit__
            self.logger.error(f"Database error during cache storage for path {path_id}: {e}", exc_info=True)
            raise CacheTransactionError(
                f"Error during cache transaction",
                details={"path_id": path_id, "error": str(e)}
            ) from e
        except Exception as e:
            self.logger.error(f"Unexpected error during caching for path {path_id}: {e}", exc_info=True)
            raise CacheError(
                f"Unexpected error during caching",
                error_code=ErrorCode.UNKNOWN_ERROR,
                details={"error": str(e)}
            ) from e

    async def _find_direct_paths(
        self, session: AsyncSession, source_ontology: str, target_ontology: str
    ) -> List[MappingPath]:
        """Find direct mapping paths from source to target ontology without direction reversal."""
        self.logger.debug(
            f"Searching for direct mapping paths from '{source_ontology}' to '{target_ontology}'"
        )

        # We need to join MappingPath -> MappingPathStep -> MappingResource
        # for both the first step (to check source_ontology)
        # and the last step (to check target_ontology).

        # Subquery to find the first step's input ontology for each path
        first_step_sq = (
            select(MappingPathStep.mapping_path_id, MappingResource.input_ontology_term)
            .join(
                MappingResource,
                MappingPathStep.mapping_resource_id == MappingResource.id,
            )
            .where(MappingPathStep.step_order == 1)
            .distinct()
            .subquery("first_step_sq")
        )

        # Subquery to find the maximum step order for each path
        max_step_sq = (
            select(
                MappingPathStep.mapping_path_id,
                func.max(MappingPathStep.step_order).label("max_order"),
            )
            .group_by(MappingPathStep.mapping_path_id)
            .subquery("max_step_sq")
        )

        # Subquery to find the last step's output ontology for each path
        last_step_sq = (
            select(
                MappingPathStep.mapping_path_id, MappingResource.output_ontology_term
            )
            .join(
                MappingResource,
                MappingPathStep.mapping_resource_id == MappingResource.id,
            )
            .join(
                max_step_sq,
                (MappingPathStep.mapping_path_id == max_step_sq.c.mapping_path_id)
                & (MappingPathStep.step_order == max_step_sq.c.max_order),
            )
            .distinct()
            .subquery("last_step_sq")
        )

        # Main query to select MappingPaths matching source and target
        stmt = (
            select(MappingPath)
            # Use selectinload for eager loading related steps and resources
            .options(
                selectinload(MappingPath.steps).joinedload(
                    MappingPathStep.mapping_resource
                )
            )
            .join(first_step_sq, MappingPath.id == first_step_sq.c.mapping_path_id)
            .join(last_step_sq, MappingPath.id == last_step_sq.c.mapping_path_id)
            .where(first_step_sq.c.input_ontology_term == source_ontology)
            .where(last_step_sq.c.output_ontology_term == target_ontology)
            .order_by(MappingPath.priority.asc())  # Lower number means higher priority
        )

        try:
            result = await session.execute(stmt)
        except SQLAlchemyError as e:
            self.logger.error(
                f"Database query error finding direct paths: {e}", exc_info=True
            )
            raise BiomapperError(
                f"Database error finding paths from {source_ontology} to {target_ontology}",
                error_code=ErrorCode.DATABASE_QUERY_ERROR,
                details={"source": source_ontology, "target": target_ontology},
            ) from e

        # Use unique() to handle potential duplicates if joins create multiple rows for the same path
        paths = result.scalars().unique().all()

        if paths:
            self.logger.debug(
                f"Found {len(paths)} direct mapping path(s) from '{source_ontology}' to '{target_ontology}'"
            )
            # Log the found paths for clarity
            for path in paths:
                self.logger.debug(
                    f" - Path ID: {path.id}, Name: '{path.name}', Priority: {path.priority}"
                )
        else:
            self.logger.debug(
                f"No direct mapping paths found from '{source_ontology}' to '{target_ontology}'"
            )

        return paths

    async def _find_mapping_paths(
        self,
        session: AsyncSession,
        source_ontology: str,
        target_ontology: str,
        bidirectional: bool = False,
    ) -> List[Union[MappingPath, "ReversiblePath"]]:
        """
        Find mapping paths between ontologies, optionally searching in both directions.

        Args:
            session: The database session
            source_ontology: Source ontology term
            target_ontology: Target ontology term
            bidirectional: If True, also search for reverse paths (target→source) when no forward paths exist

        Returns:
            List of paths (may be wrapped in ReversiblePath if reverse paths were found)
        """
        self.logger.debug(
            f"Searching for mapping paths from '{source_ontology}' to '{target_ontology}' (bidirectional={bidirectional})"
        )

        # First try to find forward paths
        forward_paths = await self._find_direct_paths(
            session, source_ontology, target_ontology
        )
        paths = [ReversiblePath(path, is_reverse=False) for path in forward_paths]

        # If bidirectional flag is set and no forward paths were found, try reverse
        if bidirectional and not paths:
            self.logger.info(
                f"No forward paths found, searching for reverse paths from '{target_ontology}' to '{source_ontology}'"
            )
            reverse_paths = await self._find_direct_paths(
                session, target_ontology, source_ontology
            )
            paths.extend(
                [ReversiblePath(path, is_reverse=True) for path in reverse_paths]
            )

        if paths:
            direction = "bidirectional" if bidirectional else "forward"
            self.logger.info(
                f"Found {len(paths)} potential mapping path(s) using {direction} search"
            )
            for path in paths:
                reverse_text = "(REVERSE)" if path.is_reverse else ""
                self.logger.info(
                    f" - Path ID: {path.id}, Name: '{path.name}' {reverse_text}, Priority: {path.priority}"
                )
        else:
            self.logger.warning(
                f"No mapping paths found from '{source_ontology}' to '{target_ontology}' (bidirectional={bidirectional})"
            )

        return paths

    async def _find_best_path(
        self,
        session: AsyncSession,
        source_type: str,
        target_type: str,
        bidirectional: bool = False,
    ) -> Optional[Union[MappingPath, ReversiblePath]]:
        """
        Find the highest priority mapping path, optionally considering reverse paths.

        Args:
            session: Database session
            source_type: Source ontology type
            target_type: Target ontology type
            bidirectional: If True, also search for reverse paths if no forward paths found

        Returns:
            The highest priority path, which might be a reverse path if bidirectional=True
        """
        paths = await self._find_mapping_paths(
            session, source_type, target_type, bidirectional=bidirectional
        )
        return paths[0] if paths else None

    async def _get_endpoint_properties(self, session: AsyncSession, endpoint_name: str) -> List[EndpointPropertyConfig]:
        """Get all property configurations for an endpoint."""
        stmt = select(EndpointPropertyConfig).join(Endpoint, EndpointPropertyConfig.endpoint_id == Endpoint.id).where(Endpoint.name == endpoint_name)
        result = await session.execute(stmt)
        return result.scalars().all()

    async def _get_ontology_preferences(self, session: AsyncSession, endpoint_name: str) -> List[OntologyPreference]:
        """Get ontology preferences for an endpoint."""
        # Join Endpoint to OntologyPreference via endpoint_id
        stmt = select(OntologyPreference).join(
            Endpoint, 
            OntologyPreference.endpoint_id == Endpoint.id
        ).where(Endpoint.name == endpoint_name)
        
        result = await session.execute(stmt)
        return result.scalars().all()

    async def _get_ontology_type(self, session: AsyncSession, endpoint_name: str, property_name: str) -> Optional[str]:
        """Retrieves the primary ontology type for a given endpoint and property name."""
        self.logger.debug(f"Getting ontology type for {endpoint_name}.{property_name}")
        try:
            # Join EndpointPropertyConfig with PropertyExtractionConfig to get the ontology type
            stmt = (
                select(PropertyExtractionConfig.ontology_type)
                .join(EndpointPropertyConfig, 
                      EndpointPropertyConfig.property_extraction_config_id == PropertyExtractionConfig.id)
                .join(Endpoint)
                .where(Endpoint.name == endpoint_name)
                .where(EndpointPropertyConfig.property_name == property_name)
                .limit(1)
            )
            result = await session.execute(stmt)
            ontology_type = result.scalar_one_or_none()
            
            if ontology_type:
                self.logger.debug(f"Found ontology type: {ontology_type}")
            else:
                self.logger.warning(f"Ontology type not found for {endpoint_name}.{property_name}")
            
            return ontology_type
        except SQLAlchemyError as e:
            self.logger.error(
                f"Database error retrieving ontology type for {endpoint_name}.{property_name}: {e}",
                exc_info=True
            )
            raise DatabaseQueryError(
                f"Database error fetching ontology type",
                details={"endpoint": endpoint_name, "property": property_name, "error": str(e)}
            ) from e
        except Exception as e:
            self.logger.error(
                f"Unexpected error retrieving ontology type for {endpoint_name}.{property_name}: {e}",
                exc_info=True
            )
            raise BiomapperError(
                f"An unexpected error occurred while retrieving ontology type",
                error_code=ErrorCode.DATABASE_QUERY_ERROR,  # Changed to DATABASE_QUERY_ERROR to match test
                details={"endpoint": endpoint_name, "property": property_name, "error": str(e)}
            ) from e

    async def _load_client_class(self, client_class_path: str) -> type:
        """Dynamically load the client class."""
        try:
            module_path, class_name = client_class_path.rsplit(".", 1)
            module = importlib.import_module(module_path)
            ClientClass = getattr(module, class_name)
            return ClientClass
        except (ImportError, AttributeError) as e:
            self.logger.error(
                f"Error loading client class '{client_class_path}': {e}", exc_info=True
            )
            raise ClientInitializationError(
                f"Could not load client class {client_class_path}",
                client_name=client_class_path.split(".")[-1] if "." in client_class_path else client_class_path,
                details={"error": str(e)}
            ) from e

    async def _load_client(self, resource: MappingResource) -> Any:
        """Loads and initializes a client instance."""
        try:
            client_class = await self._load_client_class(resource.client_class_path)
            # Parse the config template
            config_for_init = {}
            if resource.config_template:
                try:
                    config_for_init = json.loads(resource.config_template)
                except json.JSONDecodeError as json_err:
                    raise ClientInitializationError(
                        f"Invalid configuration template JSON for {resource.name}",
                        client_name=resource.name,
                        details=str(json_err),
                    )

            # Initialize the client with the config, passing it as 'config'
            client_instance = client_class(config=config_for_init)
            return client_instance
        except ImportError as e:
            self.logger.error(
                f"ImportError during client initialization for resource {resource.name}: {e}",
                exc_info=True,
            )
            raise ClientInitializationError(
                f"Import error initializing client",
                client_name=resource.name if resource else "Unknown",
                details=str(e),
            ) from e
        except AttributeError as e:
            self.logger.error(
                f"AttributeError during client initialization for resource {resource.name}: {e}",
                exc_info=True,
            )
            raise ClientInitializationError(
                f"Attribute error initializing client",
                client_name=resource.name if resource else "Unknown",
                details=str(e),
            ) from e
        except Exception as e:
            # Catch any other initialization errors
            self.logger.error(
                f"Unexpected error initializing client for resource {resource.name}: {e}",
                exc_info=True,
            )
            raise ClientInitializationError(
                f"Unexpected error initializing client",
                client_name=resource.name if resource else "Unknown",
                details=str(e),
            )

    async def _execute_mapping_step(
        self, step: MappingPathStep, input_values: List[str], is_reverse: bool = False
    ) -> Dict[str, Tuple[Optional[List[str]], Optional[str]]]:
        """
        Execute a single mapping step, handling reverse execution if needed.

        Args:
            step: The mapping step to execute
            input_values: List of input identifiers
            is_reverse: If True, execute in reverse direction (output→input)

        Returns:
            Dictionary mapping input IDs to tuples: (list of output IDs, successful source component ID or None)
        """
        try:
            client_instance = await self._load_client(step.mapping_resource)
        except ClientInitializationError:
            # Propagate initialization errors directly
            raise

        try:
            if not is_reverse:
                # Normal forward execution
                self.logger.debug(
                    f"_execute_mapping_step calling {client_instance.__class__.__name__}.map_identifiers with {len(input_values)} identifiers."
                )
                if len(input_values) < 10:
                    self.logger.debug(f"  Input sample: {input_values}")
                else:
                    self.logger.debug(f"  Input sample: {input_values[:10]}...")
                return await client_instance.map_identifiers(input_values)
            else:
                # Reverse execution - try specialized reverse method first
                if hasattr(client_instance, "reverse_map_identifiers"):
                    self.logger.debug(
                        f"Using specialized reverse_map_identifiers method for {step.mapping_resource.name}"
                    )
                    # TODO: Update clients' reverse_map_identifiers if they exist to return the tuple.
                    old_format_results = await client_instance.reverse_map_identifiers(
                        input_values
                    )
                    # Wrap result in the new tuple format, assuming None for component ID in reverse
                    return {k: (v, None) for k, v in old_format_results.items()}

                # Fall back to inverting the results of forward mapping
                self.logger.info(
                    f"Executing reverse mapping for {step.mapping_resource.name} by inverting forward results"
                )
                # Call forward mapping, which now returns the tuple format
                all_forward_results: Dict[
                    str, Tuple[Optional[List[str]], Optional[str]]
                ] = await client_instance.map_identifiers(input_values)

                # Now invert the mapping (target_id → [source_id])
                inverted_results: Dict[str, Tuple[List[str], None]] = {}
                for source_id, result_tuple in all_forward_results.items():
                    (
                        target_ids_list,
                        _,
                    ) = result_tuple  # Extract target IDs, ignore component ID for inversion
                    if not target_ids_list:
                        continue

                    for target_id in target_ids_list:
                        if (
                            target_id in input_values
                        ):  # Only include targets that were part of our original reverse input
                            if target_id not in inverted_results:
                                # Initialize with the tuple format (list, None)
                                inverted_results[target_id] = ([], None)
                            # Append source_id to the list within the tuple
                            inverted_results[target_id][0].append(source_id)

                # Add empty results (None, None) for inputs with no matches
                for input_id in input_values:
                    if input_id not in inverted_results:
                        inverted_results[input_id] = (None, None)

                return inverted_results

        except ClientError as ce:  # Catch specific client errors if raised by client
            self.logger.error(
                f"ClientError during execution step for {step.mapping_resource.name}: {ce}",
                exc_info=False, # Only log the exception message unless debug is high
            )

            # Ensure details is always a dictionary
            details_dict = (
                ce.details
                if isinstance(ce.details, dict)
                else {"error_message": str(ce.details)}
            )

            raise ClientExecutionError(
                f"Client error during step execution: {ce.message}",
                client_name=step.mapping_resource.name,
                details=details_dict,
                error_code=ErrorCode.CLIENT_EXECUTION_ERROR,
            ) from ce
        except (
            Exception
        ) as e:  # Fallback for other unexpected errors during client execution
            error_details = {"original_exception": str(e)}
            self.logger.error(
                f"Unexpected error during execution step for {step.mapping_resource.name}: {e}",
                exc_info=True,
            )
            raise ClientExecutionError(
                f"Unexpected error during step execution",
                client_name=step.mapping_resource.name,
                details=error_details,
            ) from e

    async def _get_path_details_from_log(
        self, cache_session: AsyncSession, path_log_id: int
    ) -> Dict[str, Any]:
        """
        Get path details for a given path execution log.

        Args:
            cache_session: Database session for the cache database
            path_log_id: ID of the PathExecutionLog

        Returns:
            Dict with path details (hop_count, resource types, etc.)
        """
        try:
            path_log = await cache_session.get(MappingPathExecutionLog, path_log_id)
        except SQLAlchemyError as e:
            self.logger.error(
                f"Cache retrieval error getting path log ID {path_log_id}: {e}",
                exc_info=True,
            )
            raise CacheRetrievalError(
                f"[{ErrorCode.CACHE_RETRIEVAL_ERROR}] Error during cache lookup query. (original_exception={type(e).__name__}: {e})",
                details={"log_id": path_log_id},
            ) from e

        if not path_log:
            self.logger.warning(f"PathExecutionLog with ID {path_log_id} not found")
            return {
                "hop_count": 1,
                "resource_types": ["unknown"],
                "client_identifiers": ["unknown"],
            }

        # Get the mapping path ID
        path_id = path_log.relationship_mapping_path_id

        # Create a session for metamapper DB to get full path details
        async with self.async_metamapper_session() as meta_session:
            # Use the helper method to get path details
            return await self._get_path_details(meta_session, path_id)

    async def _create_mapping_log(
        self,
        cache_session: AsyncSession,
        path_id: int,
        status: PathExecutionStatus,
        representative_source_id: str,
        source_entity_type: str,
    ) -> MappingPathExecutionLog:
        """
        Create a new path execution log entry.

        Args:
            cache_session: The cache database session
            path_id: The ID of the mapping path being executed
            status: Initial status of the path execution
            representative_source_id: A source ID to represent this execution batch
            source_entity_type: The ontology type of the source entities

        Returns:
            The created PathExecutionLog instance
        """
        try:
            now = datetime.now(timezone.utc)
            log_entry = MappingPathExecutionLog(
                relationship_mapping_path_id=path_id,
                status=status,
                start_time=now,
                source_entity_id=representative_source_id,  # Updated field name
                source_entity_type=source_entity_type,
            )
            cache_session.add(log_entry)
            await cache_session.flush()  # Ensure ID is generated
            return log_entry
        except SQLAlchemyError as e:
            self.logger.error(f"Cache storage error creating path log: {e}", exc_info=True)
            raise CacheStorageError(
                f"[{ErrorCode.CACHE_STORAGE_ERROR}] Failed to create path execution log entry. (original_exception={type(e).__name__}: {e})",
                details={"path_id": path_id, "source_id": representative_source_id},
            ) from e

    async def _check_cache(
        self,
        input_identifiers: List[str],
        source_ontology: str,
        target_ontology: str,
        mapping_path_id: Optional[int] = None,
        expiry_time: Optional[datetime] = None,
    ) -> Dict[str, Dict[str, Any]]:
        """
        Check cache for existing mapping results.

        Args:
            input_identifiers: List of identifiers to check for cached mappings
            source_ontology: The ontology type of the source entities
            target_ontology: The ontology type of the target entities
            mapping_path_id: Optional ID of the mapping path to filter results by
            expiry_time: Optional cutoff time for result freshness

        Returns:
            Dictionary mapping input identifiers to result dictionaries containing target IDs and metadata
        """
        if not input_identifiers:
            return {}

        results = {}

        try:
            async with self.async_cache_session() as cache_session:
                # Construct base query
                stmt = select(EntityMapping).where(
                    EntityMapping.source_type == source_ontology,
                    EntityMapping.target_type == target_ontology
                )

                # Add filter for source_id based on the number of identifiers
                if len(input_identifiers) == 1:
                    stmt = stmt.where(EntityMapping.source_id == input_identifiers[0])
                elif len(input_identifiers) > 1:
                    stmt = stmt.where(EntityMapping.source_id.in_(input_identifiers))
                else: # len == 0
                    return {} # No identifiers, return empty cache results

                # Add timestamp filtering if expiry_time is provided
                if expiry_time:
                    stmt = stmt.where(EntityMapping.last_updated >= expiry_time)

                try:
                    # Execute query
                    result = await cache_session.execute(stmt)
                    mappings = result.scalars().all()
                except SQLAlchemyError as e:
                    self.logger.error(f"Cache query execution failed: {e}", exc_info=True)
                    raise CacheRetrievalError(
                        f"Error during cache lookup query",
                        details={"error": str(e)}
                    ) from e
                except Exception as e:
                    self.logger.error(f"Unexpected error during cache retrieval: {e}", exc_info=True)
                    raise CacheError(
                        f"Unexpected error during cache retrieval",
                        error_code=ErrorCode.UNKNOWN_ERROR,
                        details={"error": str(e)}
                    ) from e

                # Process the mappings
                for mapping in mappings:
                    # If mapping_path_id is specified, check if it matches
                    should_include = True
                    if mapping_path_id is not None and mapping.mapping_path_details:
                        # Extract path_id from the JSON string
                        try:
                            if isinstance(mapping.mapping_path_details, str):
                                path_details = json.loads(mapping.mapping_path_details)
                            else:
                                path_details = mapping.mapping_path_details
                                
                            stored_path_id = path_details.get('path_id')
                            if stored_path_id != mapping_path_id:
                                should_include = False
                        except (json.JSONDecodeError, AttributeError, TypeError):
                            # If we can't determine the path ID, don't include this result
                            should_include = False
                    
                    if should_include:
                        # Format the result with consistent structure
                        target_identifiers = None
                        if mapping.target_id:
                            # Check if it's a JSON array
                            try:
                                if mapping.target_id.startswith('[') and mapping.target_id.endswith(']'):
                                    # It's a JSON array of target IDs
                                    target_identifiers = json.loads(mapping.target_id)
                                else:
                                    # Single target ID
                                    target_identifiers = [mapping.target_id]
                            except (json.JSONDecodeError, AttributeError):
                                # Fallback to treating as a single ID
                                target_identifiers = [mapping.target_id]
                        
                        # Get mapping path details
                        path_details = None
                        if mapping.mapping_path_details:
                            try:
                                if isinstance(mapping.mapping_path_details, str):
                                    path_details = json.loads(mapping.mapping_path_details)
                                else:
                                    path_details = mapping.mapping_path_details
                            except (json.JSONDecodeError, TypeError):
                                # If invalid, leave as None
                                pass
                                
                        # Create a result structure that matches what _execute_path returns
                        results[mapping.source_id] = {
                            "source_identifier": mapping.source_id,
                            "target_identifiers": target_identifiers,
                            "status": PathExecutionStatus.SUCCESS.value,
                            "message": "Found in cache.",
                            "confidence_score": mapping.confidence_score or 0.8,  # Default if not set
                            "mapping_path_details": path_details,
                            "hop_count": mapping.hop_count,
                            "mapping_direction": mapping.mapping_direction,
                            "cached": True,  # Flag indicating this was from cache
                        }

                return results

        except SQLAlchemyError as e:
            self.logger.error(f"Database error checking cache: {e}", exc_info=True)
            raise CacheRetrievalError(
                f"Error during cache lookup query",
                details={
                    "source_type": source_ontology,
                    "target_type": target_ontology,
                    "count": len(input_identifiers),
                    "error": str(e)
                }
            ) from e
        except Exception as e:
            self.logger.error(f"Unexpected error checking cache: {e}", exc_info=True)
            raise CacheError(
                f"Unexpected error during cache retrieval",
                error_code=ErrorCode.UNKNOWN_ERROR,
                details={"error": str(e)}
            ) from e

    async def execute_mapping(
        self,
        source_endpoint_name: str,
        target_endpoint_name: str,
        input_identifiers: List[str] = None,
        input_data: List[str] = None, # Preferred input parameter
        source_property_name: str = "PrimaryIdentifier",
        target_property_name: str = "PrimaryIdentifier",
        use_cache: bool = True,
        max_cache_age_days: Optional[int] = None,
        mapping_direction: str = "forward", # Primarily for initial path finding bias
        try_reverse_mapping: bool = False, # Allows using reversed path if no forward found
    ) -> Dict[str, Any]:
        """
        Execute a mapping process based on endpoint configurations, using an iterative strategy.

        Steps:
        1. Attempt direct mapping using the primary shared ontology.
        2. Identify unmapped entities.
        3. For unmapped entities, attempt to convert secondary identifiers to the primary shared ontology based on priority. (To be implemented next)
        4. Re-attempt direct mapping using derived primary identifiers. (To be implemented next)
        5. Aggregate results.

        :param source_endpoint_name: Source endpoint name
        :param target_endpoint_name: Target endpoint name
        :param input_identifiers: List of identifiers to map (deprecated, use input_data instead)
        :param input_data: List of identifiers to map (preferred parameter)
        :param source_property_name: Property name defining the primary ontology type for the source endpoint
        :param target_property_name: Property name defining the primary ontology type for the target endpoint
        :param use_cache: Whether to check the cache before executing mapping steps
        :param max_cache_age_days: Maximum age of cached results to use (None = no limit)
        :param mapping_direction: The preferred direction ('forward' or 'reverse') - influences path selection but strategy remains the same.
        :param try_reverse_mapping: Allows using a reversed path if no forward path found in direct/indirect steps.
        :return: Dictionary with mapping results, including provenance.
        """
        # --- Input Handling ---
        if input_data is not None and input_identifiers is None:
            input_identifiers = input_data
        elif input_identifiers is None and input_data is None:
            self.logger.warning("No input identifiers provided for mapping.")
            return {} # Return empty if no input
        # Ensure it's a list even if None was passed initially
        input_identifiers = input_identifiers if input_identifiers is not None else []

        # Use a set for efficient lookup and to handle potential duplicates in input
        original_input_ids_set = set(input_identifiers)
        successful_mappings = {}  # Store successfully mapped {input_id: result_details}
        processed_ids = set() # Track IDs processed in any successful step (cache hit or execution)
        final_results = {} # Initialize final results

        # --- 0. Initial Setup --- Create a mapping session for logging ---
        mapping_session_id = await self._create_mapping_session_log(
            source_endpoint_name, target_endpoint_name, source_property_name,
            target_property_name, use_cache, try_reverse_mapping, len(original_input_ids_set),
            max_cache_age_days=max_cache_age_days
        )

        try:
            # --- 1. Get Endpoint Config and Primary Ontologies ---
            async with self.async_metamapper_session() as meta_session:
                self.logger.info(
                    f"Executing mapping: {source_endpoint_name}.{source_property_name} -> {target_endpoint_name}.{target_property_name}"
                )

                # Fetch endpoints and primary ontology types
                source_endpoint = await self._get_endpoint(meta_session, source_endpoint_name)
                target_endpoint = await self._get_endpoint(meta_session, target_endpoint_name)
                primary_source_ontology = await self._get_ontology_type(
                    meta_session, source_endpoint_name, source_property_name
                )
                primary_target_ontology = await self._get_ontology_type(
                    meta_session, target_endpoint_name, target_property_name
                )

                # Validate configuration
                if not all([source_endpoint, target_endpoint, primary_source_ontology, primary_target_ontology]):
                    error_message = "Configuration Error: Could not determine endpoints or primary ontologies."
                    # Log specific missing items if needed
                    self.logger.error(f"{error_message} SourceEndpoint: {source_endpoint}, TargetEndpoint: {target_endpoint}, SourceOntology: {primary_source_ontology}, TargetOntology: {primary_target_ontology}")
                    raise ConfigurationError(error_message, ErrorCode.CONFIG_ERROR)

                self.logger.info(f"Primary mapping ontologies: {primary_source_ontology} -> {primary_target_ontology}")

                # --- 2. Attempt Direct Primary Mapping (Source Ontology -> Target Ontology) ---
                self.logger.info("--- Step 2: Attempting Direct Primary Mapping ---")
                primary_path = await self._find_best_path(
                    meta_session,
                    primary_source_ontology,
                    primary_target_ontology,
                    preferred_direction=mapping_direction,
                    allow_reverse=try_reverse_mapping,
                )

                if not primary_path:
                    self.logger.warning(
                        f"No direct primary mapping path found from {primary_source_ontology} to {primary_target_ontology}."
                    )
                else:
                    self.logger.info(f"Found direct primary path: {primary_path.name} (ID: {primary_path.id})")
                    # Determine which IDs need processing (not found in cache)
                    ids_to_process_step2 = list(original_input_ids_set - processed_ids)
                    if not ids_to_process_step2:
                         self.logger.info("All relevant identifiers already processed via cache. Skipping Step 2 execution.")
                    else:
                        self.logger.info(f"Executing direct primary path for {len(ids_to_process_step2)} identifiers.")
                        primary_results_details = await self._execute_path(
                            meta_session,
                            primary_path,
                            ids_to_process_step2, # Process only those not found in cache yet
                            primary_source_ontology,
                            primary_target_ontology,
                            mapping_session_id=mapping_session_id
                        )

                        # Process results from direct path
                        if primary_results_details:
                            num_newly_mapped = 0
                            for source_id, result_data in primary_results_details.items():
                                # Ensure result_data is not None and contains target_identifiers
                                if result_data and result_data.get("target_identifiers") is not None:
                                    if source_id not in successful_mappings:
                                        successful_mappings[source_id] = result_data
                                        processed_ids.add(source_id)
                                        num_newly_mapped += 1
                                    else:
                                        # Handle potential updates or conflicts if needed, though cache should prevent this
                                        self.logger.debug(f"Identifier {source_id} already mapped, skipping update from direct path.")
                            self.logger.info(f"Direct primary path execution mapped {num_newly_mapped} additional identifiers.")
                        else:
                            self.logger.info("Direct primary path execution yielded no new mappings.")

                # --- 3 & 4. Identify Unmapped Entities & Attempt Secondary -> Primary Conversion ---
                self.logger.info("--- Steps 3 & 4: Identifying Unmapped Entities & Attempting Secondary -> Primary Conversion ---")
                unmapped_ids_step3 = list(original_input_ids_set - processed_ids) # IDs not mapped by cache or Step 2

                if not unmapped_ids_step3:
                    self.logger.info("All input identifiers successfully mapped or handled in previous steps. Skipping Steps 3 & 4.")
                else:
                    self.logger.info(f"Found {len(unmapped_ids_step3)} identifiers remaining for Steps 3 & 4: {unmapped_ids_step3[:10]}...")

                    # --- 3a. Find and prioritize available secondary ontology types ---
                    # Get all available secondary properties for the source endpoint
                    all_properties = await self._get_endpoint_properties(meta_session, source_endpoint_name)
                    
                    # Filter to only secondary properties (those with different ontology than primary)
                    secondary_properties = [prop for prop in all_properties 
                                           if prop.property_name != source_property_name 
                                           and prop.ontology_type 
                                           and prop.ontology_type != primary_source_ontology]
                    
                    if not secondary_properties:
                        self.logger.warning(f"No suitable secondary properties/ontologies found for source endpoint '{source_endpoint_name}' (excluding primary '{source_property_name}' / '{primary_source_ontology}'). Skipping Steps 3 & 4.")
                    else:
                        # Get ontology preferences for the source endpoint to prioritize secondary types
                        preferences = await self._get_ontology_preferences(meta_session, source_endpoint_name)
                        
                        # Sort secondary properties by preference priority (or use order by ID if no preference found)
                        if preferences:
                            # Create a mapping of ontology_type to priority from preferences
                            priority_map = {pref.ontology_type: pref.priority for pref in preferences}
                            # Sort secondary properties by priority (lower number = higher priority)
                            secondary_properties.sort(key=lambda prop: priority_map.get(prop.ontology_type, 999))
                            self.logger.info(f"Sorted {len(secondary_properties)} secondary properties by endpoint preference priority.")
                        else:
                            self.logger.info(f"No ontology preferences found for '{source_endpoint_name}'. Using default property order.")
                            
                        # Initialize tracking for derived primary IDs
                        derived_primary_ids = {}  # Will store {source_id: {'primary_id': derived_id, 'provenance': details}}
                        
                        # --- 4. Iterate through secondary types for each unmapped entity ---
                        for secondary_prop in secondary_properties:
                            # Skip processing if all IDs now have derived primaries
                            unmapped_ids_without_derived = [uid for uid in unmapped_ids_step3 if uid not in derived_primary_ids]
                            if not unmapped_ids_without_derived:
                                self.logger.info("All unmapped identifiers now have derived primary IDs. Skipping remaining secondary properties.")
                                break
                                
                            secondary_source_ontology = secondary_prop.ontology_type
                            secondary_source_property_name = secondary_prop.property_name
                            
                            self.logger.info(f"Processing secondary property '{secondary_source_property_name}' with ontology type '{secondary_source_ontology}'")
                            self.logger.info(f"Remaining unmapped entities without derived primaries: {len(unmapped_ids_without_derived)}")
                            
                            # Find a path that converts this secondary ontology to primary source ontology
                            # This is different from before - we're looking for Secondary -> PRIMARY SOURCE (not target)
                            secondary_to_primary_path = await self._find_best_path(
                                meta_session,
                                secondary_source_ontology,  # From secondary source ontology
                                primary_source_ontology,    # To primary SOURCE ontology (not target)
                                preferred_direction=mapping_direction,
                                allow_reverse=try_reverse_mapping,
                            )
                            
                            if not secondary_to_primary_path:
                                self.logger.warning(f"No mapping path found from secondary ontology {secondary_source_ontology} to primary source ontology {primary_source_ontology}. Trying next secondary property.")
                                continue  # Try next secondary property
                                
                            self.logger.info(f"Found secondary-to-primary path: {secondary_to_primary_path.name} (ID: {secondary_to_primary_path.id})")
                            self.logger.info(f"Executing secondary-to-primary conversion for {len(unmapped_ids_without_derived)} identifiers.")
                            
                            # Execute this path to convert secondary -> primary source
                            conversion_results = await self._execute_path(
                                meta_session,
                                secondary_to_primary_path,
                                unmapped_ids_without_derived,
                                secondary_source_ontology,  # Start with secondary
                                primary_source_ontology,    # Convert to primary source
                                mapping_session_id=mapping_session_id
                            )
                            
                            # Process results - for each successfully converted ID, store the derived primary
                            if conversion_results:
                                num_newly_derived = 0
                                for source_id, result_data in conversion_results.items():
                                    if result_data and result_data.get("target_identifiers"):
                                        # Store the derived primary ID(s) for this source ID
                                        derived_primary_ids[source_id] = {
                                            "primary_ids": result_data["target_identifiers"],
                                            "provenance": {
                                                "derived_from": secondary_source_ontology,
                                                "via_path": secondary_to_primary_path.name,
                                                "path_id": secondary_to_primary_path.id,
                                                "confidence": result_data.get("confidence_score", 0.0),
                                            }
                                        }
                                        num_newly_derived += 1
                                        
                                self.logger.info(f"Derived primary IDs for {num_newly_derived} entities using {secondary_source_ontology} -> {primary_source_ontology} conversion.")
                            else:
                                self.logger.info(f"No primary IDs derived from {secondary_source_ontology} -> {primary_source_ontology} conversion.")
                                
                        self.logger.info(f"Secondary-to-primary conversion complete. Derived primary IDs for {len(derived_primary_ids)}/{len(unmapped_ids_step3)} unmapped entities.")

                # --- 5. Re-attempt Direct Primary Mapping (using derived IDs) ---
                self.logger.info("--- Step 5: Re-attempting Direct Primary Mapping using derived primary IDs ---")
                
                # Check if we have any derived primary IDs to process
                if not derived_primary_ids:
                    self.logger.info("No derived primary IDs available. Skipping Step 5.")
                else:
                    self.logger.info(f"Re-attempting primary mapping using derived IDs for {len(derived_primary_ids)} entities.")
                    
                    # Check if we have a primary path to execute
                    if not primary_path:
                        self.logger.warning(f"No direct mapping path from {primary_source_ontology} to {primary_target_ontology} available for Step 5.")
                    else:
                        # Process each derived ID separately as they may have different primary IDs
                        for source_id, derived_data in derived_primary_ids.items():
                            if source_id in processed_ids:
                                # Skip if this ID was already successfully mapped somewhere
                                continue
                                
                            derived_primary_id_list = derived_data["primary_ids"]
                            provenance_info = derived_data["provenance"]
                            
                            # For each derived primary ID, attempt the mapping to target
                            for derived_primary_id in derived_primary_id_list:
                                self.logger.debug(f"Attempting mapping for {source_id} using derived primary ID {derived_primary_id}")
                                
                                # Execute the primary path for this specific derived ID
                                derived_mapping_results = await self._execute_path(
                                    meta_session,
                                    primary_path,
                                    [derived_primary_id],  # Just the single derived ID
                                    primary_source_ontology,
                                    primary_target_ontology,
                                    mapping_session_id=mapping_session_id
                                )
                                
                                # Process results - connect back to original source ID
                                if derived_mapping_results and derived_primary_id in derived_mapping_results:
                                    result_data = derived_mapping_results[derived_primary_id]
                                    if result_data and result_data.get("target_identifiers"):
                                        # Create a merged result that maintains connection to original source ID
                                        source_result = {
                                            "source_identifier": source_id,
                                            "target_identifiers": result_data["target_identifiers"],
                                            "status": PathExecutionStatus.SUCCESS.value,
                                            "message": f"Mapped via derived primary ID {derived_primary_id}",
                                            "confidence_score": result_data.get("confidence_score", 0.5) * 0.9,  # Slightly lower confidence for indirect mapping
                                            "hop_count": (result_data.get("hop_count", 1) + 1),  # Add a hop for the derivation step
                                            "mapping_direction": result_data.get("mapping_direction", "forward"),
                                            "derived_path": True,  # Flag to indicate this was through a derived ID
                                            "intermediate_id": derived_primary_id,  # Store the intermediate ID
                                        }
                                        
                                        # Merge path details
                                        if result_data.get("mapping_path_details"):
                                            # Get path details (might be string or dict)
                                            path_details = result_data["mapping_path_details"]
                                            if isinstance(path_details, str):
                                                try:
                                                    path_details = json.loads(path_details)
                                                except json.JSONDecodeError:
                                                    path_details = {}
                                            
                                            # Add the derivation step info to path details
                                            if isinstance(path_details, dict):
                                                path_details["derived_step"] = provenance_info
                                                source_result["mapping_path_details"] = json.dumps(path_details)
                                        
                                        # Add to successful mappings
                                        successful_mappings[source_id] = source_result
                                        processed_ids.add(source_id)
                                        self.logger.debug(f"Successfully mapped {source_id} to {source_result['target_identifiers']} via derived ID {derived_primary_id}")
                                        break  # Stop processing additional derived IDs for this source once we have a success
                            
                        # Log summary of indirect mapping results
                        newly_mapped = len([sid for sid in derived_primary_ids.keys() if sid in processed_ids])
                        self.logger.info(f"Indirect mapping using derived primary IDs successfully mapped {newly_mapped}/{len(derived_primary_ids)} additional entities.")

                # --- 6. Aggregate Results & Finalize ---
                self.logger.info("--- Step 6: Aggregating final results ---")
                final_results = successful_mappings
                
                # Add nulls for any original inputs that were never successfully processed
                unmapped_count = 0
                for input_id in original_input_ids_set:
                    if input_id not in processed_ids:
                        # Use a consistent structure for not found/mapped
                        final_results[input_id] = {
                            "source_identifier": input_id,
                            "target_identifiers": None,
                            "status": PathExecutionStatus.NO_MAPPING_FOUND.value,
                            "message": "No successful mapping found via direct or secondary paths.",
                            "confidence_score": 0.0,
                            "mapping_path_details": None,
                            "hop_count": None,
                            "mapping_direction": None,
                        }
                        unmapped_count += 1
                
                self.logger.info(f"Mapping finished. Successfully processed {len(processed_ids)}/{len(original_input_ids_set)} inputs. ({unmapped_count} unmapped)")
                return final_results
                
        except BiomapperError as e:
            # Logged within specific steps or helpers typically
            self.logger.error(f"Biomapper Error during mapping execution: {e}", exc_info=True)
            # Return partial results + indicate error
            final_results = {**successful_mappings}
            error_count = 0
            for input_id in original_input_ids_set:
                if input_id not in processed_ids:
                    final_results[input_id] = {
                        "source_identifier": input_id,
                        "target_identifiers": None,
                        "status": PathExecutionStatus.ERROR.value,
                        "message": f"Mapping failed due to error: {e}",
                        # Add error details if possible/safe
                        "confidence_score": 0.0,
                        "mapping_path_details": None,
                        "hop_count": None,
                        "mapping_direction": None,
                    }
                    error_count += 1
            self.logger.warning(f"Returning partial results due to error. {error_count} inputs potentially affected.")
            return final_results
            
        except Exception as e:
            self.logger.exception("Unhandled exception during mapping execution.")
            # Re-raise as a generic mapping error? Or return error structure?
            # For now, return error structure for all non-processed IDs
            final_results = {**successful_mappings}
            error_count = 0
            for input_id in original_input_ids_set:
                if input_id not in processed_ids:
                    final_results[input_id] = {
                        "source_identifier": input_id,
                        "target_identifiers": None,
                        "status": PathExecutionStatus.ERROR.value,
                        "message": f"Unexpected error during mapping: {e}",
                        "confidence_score": 0.0,
                        "mapping_path_details": None,
                        "hop_count": None,
                        "mapping_direction": None,
                    }
                    error_count += 1
            self.logger.error(f"Unhandled exception affected {error_count} inputs.")
            return final_results
            
        finally:
            # Update session log upon completion (success, partial, or handled failure)
            if 'mapping_session_id' in locals() and mapping_session_id:
                status = PathExecutionStatus.SUCCESS
                if 'final_results' in locals():
                    # Check for error status - use string literals since we need to compare with string values
                    # PathExecutionStatus.FAILURE.value is the proper way to check error status
                    if any(r.get("status") == "failure" for r in final_results.values()):
                        status = PathExecutionStatus.PARTIAL_SUCCESS
                elif 'e' in locals():
                    status = PathExecutionStatus.FAILURE
                    
                await self._update_mapping_session_log(
                    mapping_session_id, 
                    status=status,
                    end_time=get_current_utc_time(),
                    error_message=str(e) if 'e' in locals() else None
                )
            else:
                self.logger.error("mapping_session_id not defined, cannot update session log.")

    async def _execute_path(
        self,
        session: AsyncSession, # Pass meta session
        path: Union[MappingPath, "ReversiblePath"],
        input_identifiers: List[str],
        source_ontology: str,
        target_ontology: str,
        mapping_session_id: Optional[int] = None
    ) -> Dict[str, Optional[Dict[str, Any]]]:
        """Execute a mapping path or its reverse."""
        self.logger.debug(f"Executing path {path.id} for {len(input_identifiers)} IDs")
        
        # Convert list to set for _run_path_steps
        input_ids_set = set(input_identifiers)
        
        try:
            # Call _run_path_steps to execute the path
            raw_results = await self._run_path_steps(
                path=path,
                initial_input_ids=input_ids_set,
                meta_session=session,
                mapping_session_id=mapping_session_id
            )
            
            # Transform the results into the format expected by execute_mapping
            path_results = {}
            for original_id, result_data in raw_results.items():
                if not result_data or not result_data.get('final_ids'):
                    # No mapping found for this ID
                    continue
                
                # Extract the final mapped IDs
                final_ids = result_data.get('final_ids', [])
                
                # Get provenance data (first entry if multiple exist)
                provenance = result_data.get('provenance', [{}])[0]
                
                # Extract path details from provenance
                path_id = provenance.get('path_id')
                path_name = provenance.get('path_name')
                steps_details = provenance.get('steps_details', [])
                
                # Check if any step involved historical ID resolution
                resolved_historical = any(
                    step.get('resolved_historical', False) 
                    for step in steps_details
                )
                
                # Calculate hop count (number of steps)
                hop_count = len(steps_details)
                
                # Determine mapping direction
                mapping_direction = "reverse" if getattr(path, 'is_reverse', False) else "forward"
                
                # Build the result structure
                path_results[original_id] = {
                    "source_identifier": original_id,
                    "target_identifiers": final_ids,
                    "status": PathExecutionStatus.SUCCESS.value,
                    "message": f"Successfully mapped via path: {path_name}",
                    "confidence_score": 0.9 if hop_count <= 1 else (0.8 if hop_count == 2 else 0.7),  # Simple confidence score
                    "mapping_path_details": {
                        "path_id": path_id,
                        "path_name": path_name,
                        "direction": mapping_direction,
                        "resolved_historical": resolved_historical,
                        "steps": [
                            {
                                "order": i + 1,
                                "resource_id": step.get("resource_id"),
                                "client_name": step.get("client_name"),
                                "resolved_historical": step.get("resolved_historical", False)
                            }
                            for i, step in enumerate(steps_details)
                        ]
                    },
                    "hop_count": hop_count,
                    "mapping_direction": mapping_direction,
                }
            
            return path_results
        
        except Exception as e:
            self.logger.error(f"Error executing path {path.id}: {str(e)}", exc_info=True)
            return {}  # Return empty dict on error

    def _flatten_results(self, step_results: Dict[str, Dict[str, Any]]) -> Set[str]:
        """Flatten the 'primary_ids' from client results into a unique set for the next step."""
        all_next_inputs = set()
        for input_id, result_dict in step_results.items():
            # Assuming result_dict has 'primary_ids': List[str]
            # Need robust error handling/checking here in future
            if 'primary_ids' in result_dict and isinstance(result_dict['primary_ids'], list):
                all_next_inputs.update(result_dict['primary_ids'])
            else:
                # Handle cases where mapping failed or format is unexpected
                # For now, just log or pass; depends on desired handling
                pass # Or log warning
        return all_next_inputs

    async def _run_path_steps(
        self,
        path: Union[MappingPath, "ReversiblePath"],
        initial_input_ids: Set[str],
        meta_session: AsyncSession,
        mapping_session_id: Optional[int] = None, # Added for logging
    ) -> Dict[str, Dict[str, Any]]:
        """Execute the sequence of steps defined in a mapping path."""
        self.logger.info(f"Running path '{path.name}' (ID: {path.id}, Reverse: {getattr(path, 'is_reverse', False)}) for {len(initial_input_ids)} inputs.")

        # Keep track of the original input ID -> final mapped IDs
        final_results: Dict[str, Dict[str, Any]] = {input_id: {"final_ids": [], "provenance": []} for input_id in initial_input_ids}
        # Track which original IDs map to which intermediate IDs at each step
        # Format: {step_index: {intermediate_id: {original_id1, original_id2}}} 
        traceback_mapping: Dict[int, Dict[str, Set[str]]] = { 
            0: {input_id: {input_id} for input_id in initial_input_ids}
        }

        current_input_ids = initial_input_ids
        step_execution_details = [] # For logging/provenance

        steps_to_run = path.steps # steps property handles reversal if needed
        step_order_key = lambda s: s.step_order if not getattr(path, 'is_reverse', False) else -(s.step_order or 0)
        sorted_steps = sorted(steps_to_run, key=step_order_key)

        for i, step in enumerate(sorted_steps):
            step_start_time = get_current_utc_time()
            self.logger.debug(f"  Step {i+1}/{len(sorted_steps)}: Resource '{step.mapping_resource.name}' (ID: {step.mapping_resource_id}), Order: {step.step_order}")

            if not current_input_ids:
                self.logger.warning(f"  Skipping step {i+1} as there are no input IDs from the previous step.")
                break # No inputs, path cannot continue

            try:
                client_instance = await self._load_client(step.mapping_resource)
            except (ClientInitializationError, ConfigurationError) as e:
                self.logger.error(f"[{e.error_code}] Failed to initialize client for step {i+1} (Resource ID: {step.mapping_resource_id}). Path: {path.name}. Error: {e}", exc_info=True)
                # Log path execution failure
                # await self._log_path_execution(...) # TODO: Add path logging
                raise MappingExecutionError(
                    f"Client init failed for step {i+1}", 
                    details={"path_name": path.name, "step_number": i+1}
                ) from e

            try:
                # **** CORE CLIENT CALL ****
                # Ensure client has map_identifiers method
                if not hasattr(client_instance, 'map_identifiers'):
                     raise ClientExecutionError(f"Client {client_instance.__class__.__name__} lacks map_identifiers method.", client_name=client_instance.__class__.__name__)
                
                step_results: Dict[str, Dict[str, Any]] = await client_instance.map_identifiers(
                    list(current_input_ids)
                )
                # *************************

                step_end_time = get_current_utc_time()
                duration = (step_end_time - step_start_time).total_seconds()
                self.logger.debug(f"  Step {i+1} executed in {duration:.4f}s. Got {len(step_results)} results.")

                # --- Process results and prepare for next step --- 
                next_step_inputs = set()
                current_traceback = {} # {next_id: {original_id1, ...}}
                resolved_flag_this_step = False # Track if any historical resolution occurred

                for step_input_id, result_data in step_results.items():
                    if step_input_id not in traceback_mapping[i]:
                        self.logger.warning(f"Traceback error: ID '{step_input_id}' returned by client for step {i+1} was not in the inputs for this step. Skipping.")
                        continue
                    
                    original_ids_for_this_input = traceback_mapping[i][step_input_id]

                    # --- Extract mapped IDs and check for resolution ---
                    # TODO: Make this robust based on actual client return structure
                    # Assuming structure {'primary_ids': [...], 'was_resolved': bool}
                    mapped_ids = result_data.get('primary_ids', [])
                    was_resolved = result_data.get('was_resolved', False)
                    if was_resolved:
                        resolved_flag_this_step = True
                    # -----------------------------------------------

                    if mapped_ids:
                        next_step_inputs.update(mapped_ids)
                        for mapped_id in mapped_ids:
                            if mapped_id not in current_traceback:
                                current_traceback[mapped_id] = set()
                            current_traceback[mapped_id].update(original_ids_for_this_input)
                    else:
                        # Handle failed mapping for step_input_id if needed
                        pass
                
                # Store traceback for the next iteration
                traceback_mapping[i+1] = current_traceback
                current_input_ids = next_step_inputs

                # --- Log step execution details --- 
                step_execution_details.append({
                    "step_order": step.step_order,
                    "resource_id": step.mapping_resource_id,
                    "client_name": client_instance.__class__.__name__,
                    "input_count": len(step_results), # Or size of input list? Needs refinement
                    "output_count": len(next_step_inputs),
                    "duration_seconds": duration,
                    "resolved_historical": resolved_flag_this_step # Store the flag
                })
                # ----------------------------------

            except ClientExecutionError as e:
                self.logger.error(f"[{e.error_code}] Client execution failed for step {i+1} (Resource ID: {step.mapping_resource_id}). Path: {path.name}. Error: {e}", exc_info=True)
                 # Log path execution failure
                # await self._log_path_execution(...) # TODO: Add path logging
                raise MappingExecutionError(
                    f"Client execution failed for step {i+1}", 
                    details={"path_name": path.name, "step_number": i+1}
                ) from e
            except Exception as e:
                self.logger.error(f"[{ErrorCode.UNEXPECTED_ERROR.name}] Unexpected error during step {i+1} execution (Resource ID: {step.mapping_resource_id}). Path: {path.name}. Error: {e}", exc_info=True)
                # Log path execution failure
                # await self._log_path_execution(...) # TODO: Add path logging
                raise MappingExecutionError(
                    f"Unexpected error in step {i+1}", 
                    details={"path_name": path.name, "step_number": i+1}
                ) from e
            finally:
                 # Clean up client resources if necessary (e.g., close sessions)
                if hasattr(client_instance, 'close') and asyncio.iscoroutinefunction(client_instance.close):
                    await client_instance.close()
                elif hasattr(client_instance, 'close'):
                    client_instance.close()

        # --- Final Aggregation --- 
        final_mapped_ids = current_input_ids # IDs remaining after the last step
        final_traceback = traceback_mapping.get(len(sorted_steps), {}) 

        for final_id in final_mapped_ids:
            if final_id in final_traceback:
                original_ids = final_traceback[final_id]
                for original_id in original_ids:
                    if original_id in final_results:
                        # Ensure list uniqueness if needed
                        if final_id not in final_results[original_id]['final_ids']:
                             final_results[original_id]['final_ids'].append(final_id)
                             # TODO: Enhance provenance details here 
                             final_results[original_id]['provenance'].append({ # Basic provenance
                                 "mapped_id": final_id,
                                 "path_id": path.id,
                                 "path_name": path.name,
                                 "steps_details": step_execution_details
                             })
                    else:
                         self.logger.warning(f"Original ID '{original_id}' from traceback not found in initial results dict.")
            else:
                self.logger.warning(f"Final mapped ID '{final_id}' has no traceback information.")

        # Log path execution success
        # await self._log_path_execution(...) # TODO: Add path logging
        self.logger.info(f"Path '{path.name}' finished. Produced mappings for {sum(1 for r in final_results.values() if r['final_ids'])} original inputs.")
        
        # Filter out inputs that didn't map to anything
        successful_results = {k: v for k, v in final_results.items() if v['final_ids']}
        return successful_results

    async def _create_mapping_session_log(
        self,
        source_endpoint_name: str,
        target_endpoint_name: str,
        source_property_name: str,
        target_property_name: str,
        use_cache: bool,
        try_reverse_mapping: bool,
        input_count: int,
        max_cache_age_days: Optional[int] = None,
    ) -> int:
        """Create a new mapping session log entry."""
        try:
            async with self.async_cache_session() as cache_session:
                now = get_current_utc_time()
                log_entry = MappingSession(
                    source_endpoint_name=source_endpoint_name,
                    target_endpoint_name=target_endpoint_name,
                    source_property_name=source_property_name,
                    target_property_name=target_property_name,
                    use_cache=use_cache,
                    try_reverse_mapping=try_reverse_mapping,
                    input_count=input_count,
                    max_cache_age_days=max_cache_age_days,
                    start_time=now,
                )
                cache_session.add(log_entry)
                await cache_session.flush()  # Ensure ID is generated
                return log_entry.id
        except SQLAlchemyError as e:
            self.logger.error(f"[{ErrorCode.CACHE_STORAGE_ERROR.name}] Cache storage error creating mapping session log. (original_exception={type(e).__name__}: {e})", exc_info=True)
            raise CacheStorageError(
                f"[{ErrorCode.CACHE_STORAGE_ERROR.name}] Failed to create mapping session log entry. (original_exception={type(e).__name__}: {e})",
                details={
                    "source_endpoint": source_endpoint_name,
                    "target_endpoint": target_endpoint_name,
                    "input_count": input_count,
                },
            ) from e

    async def _update_mapping_session_log(
        self,
        session_id: int,
        status: PathExecutionStatus,
        end_time: datetime,
        error_message: Optional[str] = None,
    ):
        """Update the status and end time of a mapping session log."""
        try:
            async with self.async_cache_session() as cache_session:
                log_entry = await cache_session.get(MappingSession, session_id)
                if log_entry:
                    log_entry.status = status
                    log_entry.end_time = end_time
                    if error_message:
                        log_entry.error_message = error_message
                    await cache_session.commit()
                    self.logger.info(f"Updated mapping session log ID {session_id} with status {status}")
                else:
                    self.logger.warning(f"Mapping session log ID {session_id} not found for update.")
        except SQLAlchemyError as e:
            self.logger.error(f"[{ErrorCode.CACHE_STORAGE_ERROR.name}] Cache storage error updating mapping session log. (original_exception={type(e).__name__}: {e})", exc_info=True)
            raise CacheStorageError(
                f"[{ErrorCode.CACHE_STORAGE_ERROR.name}] Failed to update mapping session log entry. (original_exception={type(e).__name__}: {e})",
                details={"session_id": session_id},
            ) from e
