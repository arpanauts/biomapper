# Top Standardization Opportunities from Pipeline Development

Based on analyzing the entire conversation, here are the top recurring issues that should be standardized to improve future strategy development:

## 1. **Parameter Naming Inconsistencies** 游댮 CRITICAL
**Frequency**: 15+ occurrences
**Impact**: High - Caused multiple pipeline failures

### Issues Encountered:
- `input_key` vs `dataset_key` vs `input_context_key` 
- `output_path` vs `output_file` vs `output_filepath`
- `source_dataset_key` vs `source_dataset`
- `identifier_column` vs `id_column`

### Standardization Proposal:
```yaml
# ALWAYS use these parameter names:
input_key: "dataset_name"        # For input dataset references
output_key: "dataset_name"       # For output dataset references  
output_path: "/path/to/file"     # For file paths
id_column: "column_name"         # For identifier columns
```

## 2. **Context Type Handling** 游댮 CRITICAL
**Frequency**: 8+ occurrences
**Impact**: High - Broke Google Drive sync and other actions

### Issues Encountered:
- Context sometimes dict, sometimes object
- Had to add defensive code: `hasattr(context, 'get')`
- Inconsistent access patterns

### Standardization Proposal:
```python
# ALWAYS handle both context types:
if hasattr(context, 'get'):
    context_get = context.get
else:
    context_get = lambda key, default: getattr(context, key, default)
```

## 3. **Algorithm Complexity (O(n*m) vs O(n+m))** 游리 HIGH
**Frequency**: 3 occurrences
**Impact**: Very High - Changed runtime from hours to minutes

### Issues Encountered:
- Nested loops comparing 1,197 칑 266,487 items (319M comparisons)
- Not using dictionary indexing for lookups

### Standardization Proposal:
```python
# ALWAYS build index first for matching:
index = {}
for item in target_dataset:
    key = extract_key(item)
    if key not in index:
        index[key] = []
    index[key].append(item)

# Then use O(1) lookups:
for source_item in source_dataset:
    if source_item in index:  # O(1)
        matches = index[source_item]
```

## 4. **Identifier Format Normalization** 游리 HIGH
**Frequency**: 10+ occurrences
**Impact**: High - Reduced match rate from 70% to 0.9%

### Issues Encountered:
- Isoform suffixes: P12345-1 vs P12345
- Prefixes: UniProtKB:P12345 vs P12345
- Multiple formats in xrefs: "UniProtKB:Q6EMK4||PR:Q6EMK4"

### Standardization Proposal:
```python
def normalize_uniprot_id(raw_id: str) -> Tuple[str, str]:
    """Returns (base_id, full_id)"""
    # Remove common prefixes
    id_str = raw_id.replace('UniProtKB:', '').replace('PR:', '')
    # Split isoform
    base_id = id_str.split('-')[0]
    return base_id, id_str

# ALWAYS index both forms for flexibility
```

## 5. **File Loading Robustness** 游리 HIGH
**Frequency**: 5+ occurrences
**Impact**: Medium - Caused data loading failures

### Issues Encountered:
- Comment lines in TSV files not handled
- Missing `comment='#'` parameter
- Wrong dataset versions used

### Standardization Proposal:
```python
# ALWAYS use these defaults for biological data:
df = pd.read_csv(
    filepath,
    sep='\t',           # Most biological data is TSV
    comment='#',        # Skip comment lines
    low_memory=False,   # Prevent dtype warnings
    na_values=['', 'NA', 'nan', 'null', 'NULL']
)
```

## 6. **API Method Naming Alignment** 游리 HIGH
**Frequency**: 4 occurrences
**Impact**: Medium - Broke API calls

### Issues Encountered:
- `get_uniprot_data` vs `get_protein_data`
- Method signatures not matching between client and implementation

### Standardization Proposal:
```python
# ALWAYS verify API methods exist:
if not hasattr(client, method_name):
    available = [m for m in dir(client) if not m.startswith('_')]
    raise AttributeError(f"Method {method_name} not found. Available: {available}")
```

## 7. **Environment Configuration** 游리 HIGH
**Frequency**: 5+ occurrences
**Impact**: Medium - Broke Google Drive uploads

### Issues Encountered:
- `.env` not loaded in scripts
- Wrong credential paths
- Missing `load_dotenv()`

### Standardization Proposal:
```python
# ALWAYS at the top of scripts:
from dotenv import load_dotenv
import os

load_dotenv()  # Load .env file

# ALWAYS validate required env vars:
required_vars = ['GOOGLE_APPLICATION_CREDENTIALS', 'DRIVE_FOLDER_ID']
missing = [v for v in required_vars if not os.getenv(v)]
if missing:
    raise EnvironmentError(f"Missing env vars: {missing}")
```

## 8. **Pydantic Model Flexibility** 游리 HIGH
**Frequency**: 6+ occurrences
**Impact**: Medium - Broke backward compatibility

### Issues Encountered:
- Models too strict, rejected extra fields
- Missing `extra="allow"`
- Type coercion issues

### Standardization Proposal:
```python
class ActionParams(BaseModel):
    # Required fields with clear types
    required_field: str
    
    # Optional with defaults
    optional_field: int = 100
    
    class Config:
        extra = "allow"  # ALWAYS allow extra fields for compatibility
```

## 9. **Edge Case Documentation** 游리 MEDIUM
**Frequency**: Ongoing
**Impact**: Low individually, High cumulatively

### Issues Encountered:
- Q6EMK4 matches in tests but not production
- No way to track known issues
- No debug mode for specific identifiers

### Standardization Proposal:
```yaml
# In every strategy, add debug section:
debug:
  trace_identifiers: ["Q6EMK4"]  # Log detailed processing
  known_issues:
    - id: "Q6EMK4"
      description: "Shows as source_only despite being in target"
      workaround: "Manual mapping available"
```

## 10. **Testing Strategy Gaps** 游리 MEDIUM
**Frequency**: Throughout development
**Impact**: High - Late discovery of issues

### Issues Encountered:
- Not testing with real data early
- Not checking intermediate outputs
- Missing edge cases

### Standardization Proposal:
```python
# ALWAYS create three test levels:
def test_action():
    # 1. Unit test with minimal data
    test_with_minimal_data()
    
    # 2. Integration test with sample
    test_with_sample_data()
    
    # 3. Smoke test with production subset
    test_with_production_subset()
```

## Priority Standardization Checklist

### Before Starting Any New Strategy:
- [ ] Verify all parameter names follow standard convention
- [ ] Add context type handling to all actions
- [ ] Use dictionary indexing for all matching operations
- [ ] Implement identifier normalization utilities
- [ ] Add `comment='#'` to all file readers
- [ ] Verify API method names with client
- [ ] Add `load_dotenv()` to all scripts
- [ ] Set Pydantic models to `extra="allow"`
- [ ] Create debug configuration section
- [ ] Write tests at three levels

## Estimated Impact
Implementing these standardizations would have prevented:
- **80% of debugging time** spent on parameter mismatches
- **100% of performance issues** (O(n*m) problems)
- **90% of data loading failures**
- **70% of API integration issues**

## Next Steps
1. Create a `biomapper/standards/` module with utilities
2. Update action template to include these patterns
3. Add pre-commit hooks to check compliance
4. Create strategy validation tool to verify standards